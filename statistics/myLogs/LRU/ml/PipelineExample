Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/01/26 13:12:26 INFO SparkContext: Running Spark version 2.2.2-SNAPSHOT
18/01/26 13:12:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/01/26 13:12:26 INFO SparkContext: Submitted application: PipelineExample
18/01/26 13:12:26 INFO SecurityManager: Changing view acls to: kderment
18/01/26 13:12:26 INFO SecurityManager: Changing modify acls to: kderment
18/01/26 13:12:26 INFO SecurityManager: Changing view acls groups to: 
18/01/26 13:12:26 INFO SecurityManager: Changing modify acls groups to: 
18/01/26 13:12:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kderment); groups with view permissions: Set(); users  with modify permissions: Set(kderment); groups with modify permissions: Set()
18/01/26 13:12:26 INFO Utils: Successfully started service 'sparkDriver' on port 46501.
18/01/26 13:12:26 INFO SparkEnv: Registering MapOutputTracker
18/01/26 13:12:26 INFO SparkEnv: Registering BlockManagerMaster
18/01/26 13:12:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/01/26 13:12:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/01/26 13:12:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1051538e-8ace-47bd-b3fa-24f20a3cb9f9
18/01/26 13:12:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/01/26 13:12:26 INFO SparkEnv: Registering OutputCommitCoordinator
18/01/26 13:12:26 INFO log: Logging initialized @1319ms
18/01/26 13:12:26 INFO Server: jetty-9.3.11.v20160721
18/01/26 13:12:26 INFO Server: Started @1378ms
18/01/26 13:12:26 INFO AbstractConnector: Started ServerConnector@54a3ab8f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/01/26 13:12:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@420bc288{/jobs,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1556f2dd{/jobs/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@62577d6{/jobs/job,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@772485dd{/jobs/job/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79ab3a71{/stages,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3d829787{/stages/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@51bde877{/stages/stage,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2fb68ec6{/stages/stage/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3add81c4{/stages/pool,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1c65121{/stages/pool/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@57dc9128{/storage,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17ae98d7{/storage/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ac4944a{/storage/rdd,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@39fc6b2c{/storage/rdd/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ee39da0{/environment,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cc9ce8{/environment/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c0b41d6{/executors,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2bffa76d{/executors/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d2260db{/executors/threadDump,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49bf29c6{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3fcdcf{/static,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b4ef7{/,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5987e932{/api,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a8b5227{/jobs/job/kill,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6979efad{/stages/stage/kill,null,AVAILABLE,@Spark}
18/01/26 13:12:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://138.96.200.169:4040
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/htrace-core-3.0.4.jar at spark://138.96.200.169:46501/jars/htrace-core-3.0.4.jar with timestamp 1516968746928
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.annotation-api-1.2.jar at spark://138.96.200.169:46501/jars/javax.annotation-api-1.2.jar with timestamp 1516968746928
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/osgi-resource-locator-1.0.1.jar at spark://138.96.200.169:46501/jars/osgi-resource-locator-1.0.1.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-ipc-1.7.7.jar at spark://138.96.200.169:46501/jars/avro-ipc-1.7.7.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-library-2.11.8.jar at spark://138.96.200.169:46501/jars/scala-library-2.11.8.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-configuration-1.6.jar at spark://138.96.200.169:46501/jars/commons-configuration-1.6.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/ST4-4.0.4.jar at spark://138.96.200.169:46501/jars/ST4-4.0.4.jar with timestamp 1516968746929
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xercesImpl-2.9.1.jar at spark://138.96.200.169:46501/jars/xercesImpl-2.9.1.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jcl-over-slf4j-1.7.16.jar at spark://138.96.200.169:46501/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jets3t-0.9.3.jar at spark://138.96.200.169:46501/jars/jets3t-0.9.3.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/breeze-macros_2.11-0.13.2.jar at spark://138.96.200.169:46501/jars/breeze-macros_2.11-0.13.2.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.inject-2.4.0-b34.jar at spark://138.96.200.169:46501/jars/javax.inject-2.4.0-b34.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/compress-lzf-1.0.3.jar at spark://138.96.200.169:46501/jars/compress-lzf-1.0.3.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/validation-api-1.1.0.Final.jar at spark://138.96.200.169:46501/jars/validation-api-1.1.0.Final.jar with timestamp 1516968746930
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-dbcp-1.4.jar at spark://138.96.200.169:46501/jars/commons-dbcp-1.4.jar with timestamp 1516968746931
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/JavaEWAH-0.3.2.jar at spark://138.96.200.169:46501/jars/JavaEWAH-0.3.2.jar with timestamp 1516968746931
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746931
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-servlet-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-servlet-9.3.11.v20160721.jar with timestamp 1516968746931
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scopt_2.11-3.3.0.jar at spark://138.96.200.169:46501/jars/scopt_2.11-3.3.0.jar with timestamp 1516968746931
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/chill_2.11-0.8.0.jar at spark://138.96.200.169:46501/jars/chill_2.11-0.8.0.jar with timestamp 1516968746931
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-container-servlet-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-container-servlet-2.22.2.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/servlet-api-2.5-20110124.jar at spark://138.96.200.169:46501/jars/servlet-api-2.5-20110124.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-client-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-client-9.3.11.v20160721.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/calcite-linq4j-1.2.0-incubating.jar at spark://138.96.200.169:46501/jars/calcite-linq4j-1.2.0-incubating.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-beanutils-core-1.8.0.jar at spark://138.96.200.169:46501/jars/commons-beanutils-core-1.8.0.jar with timestamp 1516968746932
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/java-xmlbuilder-1.0.jar at spark://138.96.200.169:46501/jars/java-xmlbuilder-1.0.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-hdfs-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-hdfs-2.6.5.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stax-api-1.0.1.jar at spark://138.96.200.169:46501/jars/stax-api-1.0.1.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jtransforms-2.4.0.jar at spark://138.96.200.169:46501/jars/jtransforms-2.4.0.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-common-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-common-2.6.5.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-sdk-1.6.0.jar at spark://138.96.200.169:46501/jars/flume-ng-sdk-1.6.0.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/pyrolite-4.13.jar at spark://138.96.200.169:46501/jars/pyrolite-4.13.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javassist-3.18.1-GA.jar at spark://138.96.200.169:46501/jars/javassist-3.18.1-GA.jar with timestamp 1516968746933
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jettison-1.1.jar at spark://138.96.200.169:46501/jars/jettison-1.1.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-io-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-io-9.3.11.v20160721.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/paranamer-2.6.jar at spark://138.96.200.169:46501/jars/paranamer-2.6.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-module-paranamer-2.6.5.jar at spark://138.96.200.169:46501/jars/jackson-module-paranamer-2.6.5.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/ivy-2.4.0.jar at spark://138.96.200.169:46501/jars/ivy-2.4.0.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-crypto-1.0.0.jar at spark://138.96.200.169:46501/jars/commons-crypto-1.0.0.jar with timestamp 1516968746934
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-app-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-app-2.6.5.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/httpclient-4.5.2.jar at spark://138.96.200.169:46501/jars/httpclient-4.5.2.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/curator-recipes-2.6.0.jar at spark://138.96.200.169:46501/jars/curator-recipes-2.6.0.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/libfb303-0.9.3.jar at spark://138.96.200.169:46501/jars/libfb303-0.9.3.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-media-jaxb-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/api-util-1.0.0-M20.jar at spark://138.96.200.169:46501/jars/api-util-1.0.0-M20.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-json-3.1.2.jar at spark://138.96.200.169:46501/jars/metrics-json-3.1.2.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hive-exec-1.2.1.spark2.jar at spark://138.96.200.169:46501/jars/hive-exec-1.2.1.spark2.jar with timestamp 1516968746935
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-util-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-util-9.3.11.v20160721.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spire_2.11-0.13.0.jar at spark://138.96.200.169:46501/jars/spire_2.11-0.13.0.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hk2-locator-2.4.0-b34.jar at spark://138.96.200.169:46501/jars/hk2-locator-2.4.0-b34.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/velocity-1.7.jar at spark://138.96.200.169:46501/jars/velocity-1.7.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-core-1.6.0.jar at spark://138.96.200.169:46501/jars/flume-ng-core-1.6.0.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-io-2.4.jar at spark://138.96.200.169:46501/jars/commons-io-2.4.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-container-servlet-core-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1516968746936
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/apache-log4j-extras-1.2.17.jar at spark://138.96.200.169:46501/jars/apache-log4j-extras-1.2.17.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-reflect-2.11.8.jar at spark://138.96.200.169:46501/jars/scala-reflect-2.11.8.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.servlet-api-3.1.0.jar at spark://138.96.200.169:46501/jars/javax.servlet-api-3.1.0.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-encoding-1.8.2.jar at spark://138.96.200.169:46501/jars/parquet-encoding-1.8.2.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-server-common-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-yarn-server-common-2.6.5.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-xc-1.9.13.jar at spark://138.96.200.169:46501/jars/jackson-xc-1.9.13.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-pool-1.5.4.jar at spark://138.96.200.169:46501/jars/commons-pool-1.5.4.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-databind-2.6.5.jar at spark://138.96.200.169:46501/jars/jackson-databind-2.6.5.jar with timestamp 1516968746937
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/mina-core-2.0.4.jar at spark://138.96.200.169:46501/jars/mina-core-2.0.4.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-api-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-yarn-api-2.6.5.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/gson-2.2.4.jar at spark://138.96.200.169:46501/jars/gson-2.2.4.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/datanucleus-api-jdo-3.2.6.jar at spark://138.96.200.169:46501/jars/datanucleus-api-jdo-3.2.6.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-compiler-3.0.7.jar at spark://138.96.200.169:46501/jars/commons-compiler-3.0.7.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-common-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-yarn-common-2.6.5.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-core-asl-1.9.13.jar at spark://138.96.200.169:46501/jars/jackson-core-asl-1.9.13.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-collections-3.2.2.jar at spark://138.96.200.169:46501/jars/commons-collections-3.2.2.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-logging-1.2.jar at spark://138.96.200.169:46501/jars/commons-logging-1.2.jar with timestamp 1516968746938
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-common-1.8.2.jar at spark://138.96.200.169:46501/jars/parquet-common-1.8.2.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/unused-1.0.0.jar at spark://138.96.200.169:46501/jars/unused-1.0.0.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-digester-1.8.jar at spark://138.96.200.169:46501/jars/commons-digester-1.8.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/bcprov-jdk15on-1.51.jar at spark://138.96.200.169:46501/jars/bcprov-jdk15on-1.51.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stream-2.7.0.jar at spark://138.96.200.169:46501/jars/stream-2.7.0.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jdo-api-3.0.1.jar at spark://138.96.200.169:46501/jars/jdo-api-3.0.1.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/guava-14.0.1.jar at spark://138.96.200.169:46501/jars/guava-14.0.1.jar with timestamp 1516968746939
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-1.7.7.jar at spark://138.96.200.169:46501/jars/avro-1.7.7.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hk2-utils-2.4.0-b34.jar at spark://138.96.200.169:46501/jars/hk2-utils-2.4.0-b34.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-server-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-server-9.3.11.v20160721.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.inject-1.jar at spark://138.96.200.169:46501/jars/javax.inject-1.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xmlenc-0.52.jar at spark://138.96.200.169:46501/jars/xmlenc-0.52.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-hadoop-bundle-1.6.0.jar at spark://138.96.200.169:46501/jars/parquet-hadoop-bundle-1.6.0.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/shapeless_2.11-2.3.2.jar at spark://138.96.200.169:46501/jars/shapeless_2.11-2.3.2.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-6.1.26.jar at spark://138.96.200.169:46501/jars/jetty-6.1.26.jar with timestamp 1516968746940
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-plus-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-plus-9.3.11.v20160721.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-lang3-3.5.jar at spark://138.96.200.169:46501/jars/commons-lang3-3.5.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/aopalliance-1.0.jar at spark://138.96.200.169:46501/jars/aopalliance-1.0.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jaxb-api-2.2.2.jar at spark://138.96.200.169:46501/jars/jaxb-api-2.2.2.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/curator-client-2.6.0.jar at spark://138.96.200.169:46501/jars/curator-client-2.6.0.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-core-3.1.2.jar at spark://138.96.200.169:46501/jars/metrics-core-3.1.2.jar with timestamp 1516968746941
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-cli-1.2.jar at spark://138.96.200.169:46501/jars/commons-cli-1.2.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spire-macros_2.11-0.13.0.jar at spark://138.96.200.169:46501/jars/spire-macros_2.11-0.13.0.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-jvm-3.1.2.jar at spark://138.96.200.169:46501/jars/metrics-jvm-3.1.2.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-column-1.8.2.jar at spark://138.96.200.169:46501/jars/parquet-column-1.8.2.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/janino-3.0.7.jar at spark://138.96.200.169:46501/jars/janino-3.0.7.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/guice-3.0.jar at spark://138.96.200.169:46501/jars/guice-3.0.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-ipc-1.7.7-tests.jar at spark://138.96.200.169:46501/jars/avro-ipc-1.7.7-tests.jar with timestamp 1516968746942
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/mail-1.4.7.jar at spark://138.96.200.169:46501/jars/mail-1.4.7.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-client-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-client-2.22.2.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/pmml-schema-1.2.15.jar at spark://138.96.200.169:46501/jars/pmml-schema-1.2.15.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/py4j-0.10.4.jar at spark://138.96.200.169:46501/jars/py4j-0.10.4.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-auth-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-auth-2.6.5.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/bonecp-0.8.0.RELEASE.jar at spark://138.96.200.169:46501/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-client-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-client-2.6.5.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-mapper-asl-1.9.13.jar at spark://138.96.200.169:46501/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/antlr-runtime-3.4.jar at spark://138.96.200.169:46501/jars/antlr-runtime-3.4.jar with timestamp 1516968746943
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-guava-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-guava-2.22.2.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-annotations-2.6.5.jar at spark://138.96.200.169:46501/jars/jackson-annotations-2.6.5.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scalap-2.11.8.jar at spark://138.96.200.169:46501/jars/scalap-2.11.8.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-common-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-common-2.22.2.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-mapred-1.7.7-hadoop2.jar at spark://138.96.200.169:46501/jars/avro-mapred-1.7.7-hadoop2.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/RoaringBitmap-0.5.11.jar at spark://138.96.200.169:46501/jars/RoaringBitmap-0.5.11.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/objenesis-2.1.jar at spark://138.96.200.169:46501/jars/objenesis-2.1.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stax-api-1.0-2.jar at spark://138.96.200.169:46501/jars/stax-api-1.0-2.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746944
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/univocity-parsers-2.2.1.jar at spark://138.96.200.169:46501/jars/univocity-parsers-2.2.1.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-graphite-3.1.2.jar at spark://138.96.200.169:46501/jars/metrics-graphite-3.1.2.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/activation-1.1.1.jar at spark://138.96.200.169:46501/jars/activation-1.1.1.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/mx4j-3.0.2.jar at spark://138.96.200.169:46501/jars/mx4j-3.0.2.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/minlog-1.3.0.jar at spark://138.96.200.169:46501/jars/minlog-1.3.0.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/protobuf-java-2.5.0.jar at spark://138.96.200.169:46501/jars/protobuf-java-2.5.0.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/pmml-model-1.2.15.jar at spark://138.96.200.169:46501/jars/pmml-model-1.2.15.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/base64-2.3.8.jar at spark://138.96.200.169:46501/jars/base64-2.3.8.jar with timestamp 1516968746945
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-core-2.2.0.jar at spark://138.96.200.169:46501/jars/metrics-core-2.2.0.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/antlr-2.7.7.jar at spark://138.96.200.169:46501/jars/antlr-2.7.7.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/chill-java-0.8.0.jar at spark://138.96.200.169:46501/jars/chill-java-0.8.0.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/netty-all-4.0.43.Final.jar at spark://138.96.200.169:46501/jars/netty-all-4.0.43.Final.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/slf4j-api-1.7.21.jar at spark://138.96.200.169:46501/jars/slf4j-api-1.7.21.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/cglib-2.2.1-v20090111.jar at spark://138.96.200.169:46501/jars/cglib-2.2.1-v20090111.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-http-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-http-9.3.11.v20160721.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/slf4j-log4j12-1.7.16.jar at spark://138.96.200.169:46501/jars/slf4j-log4j12-1.7.16.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-hadoop-1.8.2.jar at spark://138.96.200.169:46501/jars/parquet-hadoop-1.8.2.jar with timestamp 1516968746946
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hk2-api-2.4.0-b34.jar at spark://138.96.200.169:46501/jars/hk2-api-2.4.0-b34.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/core-1.1.2.jar at spark://138.96.200.169:46501/jars/core-1.1.2.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/api-asn1-api-1.0.0-M20.jar at spark://138.96.200.169:46501/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/json4s-jackson_2.11-3.2.11.jar at spark://138.96.200.169:46501/jars/json4s-jackson_2.11-3.2.11.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-net-3.1.jar at spark://138.96.200.169:46501/jars/commons-net-3.1.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-compiler-1.7.3.jar at spark://138.96.200.169:46501/jars/avro-compiler-1.7.3.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-webapp-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-webapp-9.3.11.v20160721.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-jndi-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-jndi-9.3.11.v20160721.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746947
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jsr305-1.3.9.jar at spark://138.96.200.169:46501/jars/jsr305-1.3.9.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-auth-1.6.0.jar at spark://138.96.200.169:46501/jars/flume-ng-auth-1.6.0.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-lang-2.6.jar at spark://138.96.200.169:46501/jars/commons-lang-2.6.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/apacheds-i18n-2.0.0-M15.jar at spark://138.96.200.169:46501/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-continuation-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-continuation-9.3.11.v20160721.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-format-2.3.1.jar at spark://138.96.200.169:46501/jars/parquet-format-2.3.1.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/datanucleus-rdbms-3.2.9.jar at spark://138.96.200.169:46501/jars/datanucleus-rdbms-3.2.9.jar with timestamp 1516968746948
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jul-to-slf4j-1.7.16.jar at spark://138.96.200.169:46501/jars/jul-to-slf4j-1.7.16.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/datanucleus-core-3.2.10.jar at spark://138.96.200.169:46501/jars/datanucleus-core-3.2.10.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hive-metastore-1.2.1.spark2.jar at spark://138.96.200.169:46501/jars/hive-metastore-1.2.1.spark2.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-annotations-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-annotations-2.6.5.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jodd-core-3.5.2.jar at spark://138.96.200.169:46501/jars/jodd-core-3.5.2.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/json4s-ast_2.11-3.2.11.jar at spark://138.96.200.169:46501/jars/json4s-ast_2.11-3.2.11.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-util-6.1.26.jar at spark://138.96.200.169:46501/jars/jetty-util-6.1.26.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-compress-1.4.1.jar at spark://138.96.200.169:46501/jars/commons-compress-1.4.1.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/zkclient-0.3.jar at spark://138.96.200.169:46501/jars/zkclient-0.3.jar with timestamp 1516968746949
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/macro-compat_2.11-1.1.1.jar at spark://138.96.200.169:46501/jars/macro-compat_2.11-1.1.1.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-xml_2.11-1.0.4.jar at spark://138.96.200.169:46501/jars/scala-xml_2.11-1.0.4.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-parser-combinators_2.11-1.0.4.jar at spark://138.96.200.169:46501/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/eigenbase-properties-1.1.5.jar at spark://138.96.200.169:46501/jars/eigenbase-properties-1.1.5.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xz-1.0.jar at spark://138.96.200.169:46501/jars/xz-1.0.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.ws.rs-api-2.0.1.jar at spark://138.96.200.169:46501/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/zookeeper-3.4.6.jar at spark://138.96.200.169:46501/jars/zookeeper-3.4.6.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/json4s-core_2.11-3.2.11.jar at spark://138.96.200.169:46501/jars/json4s-core_2.11-3.2.11.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-module-scala_2.11-2.6.5.jar at spark://138.96.200.169:46501/jars/jackson-module-scala_2.11-2.6.5.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/antlr4-runtime-4.5.3.jar at spark://138.96.200.169:46501/jars/antlr4-runtime-4.5.3.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-client-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-yarn-client-2.6.5.jar with timestamp 1516968746950
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/oro-2.0.8.jar at spark://138.96.200.169:46501/jars/oro-2.0.8.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/libthrift-0.9.3.jar at spark://138.96.200.169:46501/jars/libthrift-0.9.3.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-proxy-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-proxy-9.3.11.v20160721.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-beanutils-1.7.0.jar at spark://138.96.200.169:46501/jars/commons-beanutils-1.7.0.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/joda-time-2.9.3.jar at spark://138.96.200.169:46501/jars/joda-time-2.9.3.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xbean-asm5-shaded-4.4.jar at spark://138.96.200.169:46501/jars/xbean-asm5-shaded-4.4.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/derby-10.12.1.1.jar at spark://138.96.200.169:46501/jars/derby-10.12.1.1.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-server-2.22.2.jar at spark://138.96.200.169:46501/jars/jersey-server-2.22.2.jar with timestamp 1516968746951
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/kryo-shaded-3.0.3.jar at spark://138.96.200.169:46501/jars/kryo-shaded-3.0.3.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-math3-3.4.1.jar at spark://138.96.200.169:46501/jars/commons-math3-3.4.1.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/calcite-core-1.2.0-incubating.jar at spark://138.96.200.169:46501/jars/calcite-core-1.2.0-incubating.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-xml-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-xml-9.3.11.v20160721.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-httpclient-3.1.jar at spark://138.96.200.169:46501/jars/commons-httpclient-3.1.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/opencsv-2.3.jar at spark://138.96.200.169:46501/jars/opencsv-2.3.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xml-apis-1.3.04.jar at spark://138.96.200.169:46501/jars/xml-apis-1.3.04.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-servlets-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-servlets-9.3.11.v20160721.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-security-9.3.11.v20160721.jar at spark://138.96.200.169:46501/jars/jetty-security-9.3.11.v20160721.jar with timestamp 1516968746952
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-core-2.6.5.jar at spark://138.96.200.169:46501/jars/jackson-core-2.6.5.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jline-0.9.94.jar at spark://138.96.200.169:46501/jars/jline-0.9.94.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/lz4-1.3.0.jar at spark://138.96.200.169:46501/jars/lz4-1.3.0.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/aopalliance-repackaged-2.4.0-b34.jar at spark://138.96.200.169:46501/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/arpack_combined_all-0.1.jar at spark://138.96.200.169:46501/jars/arpack_combined_all-0.1.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/snappy-0.2.jar at spark://138.96.200.169:46501/jars/snappy-0.2.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/httpcore-4.4.4.jar at spark://138.96.200.169:46501/jars/httpcore-4.4.4.jar with timestamp 1516968746953
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-codec-1.10.jar at spark://138.96.200.169:46501/jars/commons-codec-1.10.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-jaxrs-1.9.13.jar at spark://138.96.200.169:46501/jars/jackson-jaxrs-1.9.13.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/snappy-java-1.1.2.6.jar at spark://138.96.200.169:46501/jars/snappy-java-1.1.2.6.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/breeze_2.11-0.13.2.jar at spark://138.96.200.169:46501/jars/breeze_2.11-0.13.2.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-configuration-1.6.0.jar at spark://138.96.200.169:46501/jars/flume-ng-configuration-1.6.0.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javolution-5.5.1.jar at spark://138.96.200.169:46501/jars/javolution-5.5.1.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/kafka_2.11-0.8.2.1.jar at spark://138.96.200.169:46501/jars/kafka_2.11-0.8.2.1.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/netty-3.9.9.Final.jar at spark://138.96.200.169:46501/jars/netty-3.9.9.Final.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/machinist_2.11-0.6.1.jar at spark://138.96.200.169:46501/jars/machinist_2.11-0.6.1.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-compiler-2.11.8.jar at spark://138.96.200.169:46501/jars/scala-compiler-2.11.8.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-jackson-1.8.2.jar at spark://138.96.200.169:46501/jars/parquet-jackson-1.8.2.jar with timestamp 1516968746954
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/leveldbjni-all-1.8.jar at spark://138.96.200.169:46501/jars/leveldbjni-all-1.8.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stringtemplate-3.2.1.jar at spark://138.96.200.169:46501/jars/stringtemplate-3.2.1.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/apacheds-kerberos-codec-2.0.0-M15.jar at spark://138.96.200.169:46501/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/calcite-avatica-1.2.0-incubating.jar at spark://138.96.200.169:46501/jars/calcite-avatica-1.2.0-incubating.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:46501/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-common-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-common-2.6.5.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/curator-framework-2.6.0.jar at spark://138.96.200.169:46501/jars/curator-framework-2.6.0.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jta-1.1.jar at spark://138.96.200.169:46501/jars/jta-1.1.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/kafka-clients-0.10.0.1.jar at spark://138.96.200.169:46501/jars/kafka-clients-0.10.0.1.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-core-2.6.5.jar at spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-core-2.6.5.jar with timestamp 1516968746955
18/01/26 13:12:26 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/log4j-1.2.17.jar at spark://138.96.200.169:46501/jars/log4j-1.2.17.jar with timestamp 1516968746956
18/01/26 13:12:27 INFO Executor: Starting executor ID driver on host localhost
18/01/26 13:12:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42291.
18/01/26 13:12:27 INFO NettyBlockTransferService: Server created on 138.96.200.169:42291
18/01/26 13:12:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/01/26 13:12:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 138.96.200.169, 42291, None)
18/01/26 13:12:27 INFO BlockManagerMasterEndpoint: Registering block manager 138.96.200.169:42291 with 366.3 MB RAM, BlockManagerId(driver, 138.96.200.169, 42291, None)
18/01/26 13:12:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 138.96.200.169, 42291, None)
18/01/26 13:12:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 138.96.200.169, 42291, None)
18/01/26 13:12:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c0884e8{/metrics/json,null,AVAILABLE,@Spark}
18/01/26 13:12:27 INFO EventLoggingListener: Logging events to file:/workspaces/kderment/repos/spark/logs/local-1516968746983
18/01/26 13:12:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/local/workspaces/kderment/repos/spark/bin/spark-warehouse/').
18/01/26 13:12:28 INFO SharedState: Warehouse path is 'file:/local/workspaces/kderment/repos/spark/bin/spark-warehouse/'.
18/01/26 13:12:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@ab2e6d2{/SQL,null,AVAILABLE,@Spark}
18/01/26 13:12:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d4436d0{/SQL/json,null,AVAILABLE,@Spark}
18/01/26 13:12:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6010d0d1{/SQL/execution,null,AVAILABLE,@Spark}
18/01/26 13:12:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5348d83c{/SQL/execution/json,null,AVAILABLE,@Spark}
18/01/26 13:12:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ba42204{/static/sql,null,AVAILABLE,@Spark}
18/01/26 13:12:28 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
18/01/26 13:12:29 INFO CodeGenerator: Code generated in 159.86217 ms
18/01/26 13:12:29 INFO Instrumentation: LogisticRegression-logreg_ab3e78168e03-1079214960-1: training: numPartitions=4 storageLevel=StorageLevel(disk, memory, deserialized, 1 replicas)
18/01/26 13:12:29 INFO Instrumentation: LogisticRegression-logreg_ab3e78168e03-1079214960-1: {"regParam":0.001,"maxIter":10}
18/01/26 13:12:29 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:517
18/01/26 13:12:29 INFO DAGScheduler: Got job 0 (treeAggregate at LogisticRegression.scala:517) with 4 output partitions
18/01/26 13:12:29 INFO DAGScheduler: Final stage: ResultStage 0 (treeAggregate at LogisticRegression.scala:517)
18/01/26 13:12:29 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:29 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at treeAggregate at LogisticRegression.scala:517), which has no missing parents
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||   id: 0,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 0,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 5,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     {id: 5,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[5] at treeAggregate at LogisticRegression.scala:517,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:29 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:29 WARN Simulator: || SIMULATION || Starting for job = 0
18/01/26 13:12:29 WARN Simulator: || SIMULATION || submitStage(ResultStage 0)
18/01/26 13:12:29 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:29 WARN Simulator: || SIMULATION || Statistics for 0:
18/01/26 13:12:29 WARN Simulator: || SIMULATION || hits = 0
18/01/26 13:12:29 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:29 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:29 WARN Simulator: || SIMULATION || narrowDependencies = 5
18/01/26 13:12:29 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:29 WARN Simulator: || SIMULATION || Finished for job = 0
18/01/26 13:12:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.2 KB, free 366.3 MB)
18/01/26 13:12:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.6 KB, free 366.3 MB)
18/01/26 13:12:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 138.96.200.169:42291 (size: 4.6 KB, free: 366.3 MB)
18/01/26 13:12:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at treeAggregate at LogisticRegression.scala:517) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
18/01/26 13:12:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:29 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:29 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:29 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:29 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
18/01/26 13:12:29 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
18/01/26 13:12:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/01/26 13:12:29 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-collections-3.2.2.jar with timestamp 1516968746938
18/01/26 13:12:29 INFO TransportClientFactory: Successfully created connection to /138.96.200.169:46501 after 18 ms (0 ms spent in bootstraps)
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-collections-3.2.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1283940790427970330.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-collections-3.2.2.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/libthrift-0.9.3.jar with timestamp 1516968746951
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/libthrift-0.9.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8266867365144272861.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/libthrift-0.9.3.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/stax-api-1.0.1.jar with timestamp 1516968746933
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/stax-api-1.0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7728821693113036787.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/stax-api-1.0.1.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746932
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp71616748216637022.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-launcher_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/JavaEWAH-0.3.2.jar with timestamp 1516968746931
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/JavaEWAH-0.3.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8278088790872174866.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/JavaEWAH-0.3.2.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/netty-3.9.9.Final.jar with timestamp 1516968746954
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/netty-3.9.9.Final.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp256522283501836185.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/netty-3.9.9.Final.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/janino-3.0.7.jar with timestamp 1516968746942
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/janino-3.0.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3955535833443924043.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/janino-3.0.7.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/calcite-linq4j-1.2.0-incubating.jar with timestamp 1516968746932
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/calcite-linq4j-1.2.0-incubating.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7510553386284857188.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/calcite-linq4j-1.2.0-incubating.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/opencsv-2.3.jar with timestamp 1516968746952
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/opencsv-2.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5597840851950847374.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/opencsv-2.3.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-httpclient-3.1.jar with timestamp 1516968746952
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-httpclient-3.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5207495742765327919.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-httpclient-3.1.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/stream-2.7.0.jar with timestamp 1516968746939
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/stream-2.7.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2516030186502671206.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/stream-2.7.0.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/avro-ipc-1.7.7-tests.jar with timestamp 1516968746942
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/avro-ipc-1.7.7-tests.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4353459176253925274.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/avro-ipc-1.7.7-tests.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/json4s-ast_2.11-3.2.11.jar with timestamp 1516968746949
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/json4s-ast_2.11-3.2.11.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8877605650338265699.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/json4s-ast_2.11-3.2.11.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/unused-1.0.0.jar with timestamp 1516968746939
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/unused-1.0.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp302990707629370221.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/unused-1.0.0.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/metrics-core-3.1.2.jar with timestamp 1516968746941
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/metrics-core-3.1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp928689127238038512.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/metrics-core-3.1.2.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/arpack_combined_all-0.1.jar with timestamp 1516968746953
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/arpack_combined_all-0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2663624630512457051.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/arpack_combined_all-0.1.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-jndi-9.3.11.v20160721.jar with timestamp 1516968746947
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-jndi-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7692882961359645076.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-jndi-9.3.11.v20160721.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/chill_2.11-0.8.0.jar with timestamp 1516968746931
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/chill_2.11-0.8.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4743717325129513029.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/chill_2.11-0.8.0.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jul-to-slf4j-1.7.16.jar with timestamp 1516968746949
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jul-to-slf4j-1.7.16.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7911588223732565066.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jul-to-slf4j-1.7.16.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/gson-2.2.4.jar with timestamp 1516968746938
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/gson-2.2.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2200549078211074876.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/gson-2.2.4.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jdo-api-3.0.1.jar with timestamp 1516968746939
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jdo-api-3.0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2148631087466061863.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jdo-api-3.0.1.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jtransforms-2.4.0.jar with timestamp 1516968746933
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jtransforms-2.4.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4378693592388549475.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jtransforms-2.4.0.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/antlr4-runtime-4.5.3.jar with timestamp 1516968746950
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/antlr4-runtime-4.5.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7634312898492091205.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/antlr4-runtime-4.5.3.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scala-compiler-2.11.8.jar with timestamp 1516968746954
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scala-compiler-2.11.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7145547677278267479.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scala-compiler-2.11.8.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-client-2.22.2.jar with timestamp 1516968746943
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-client-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3976485565096355534.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-client-2.22.2.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746945
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8578614887731769119.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-sql_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746947
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3211247530737725479.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746931
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1662443225422831254.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-mllib_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/osgi-resource-locator-1.0.1.jar with timestamp 1516968746929
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/osgi-resource-locator-1.0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6919968357822529770.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/osgi-resource-locator-1.0.1.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746948
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8000631546762802218.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-proxy-9.3.11.v20160721.jar with timestamp 1516968746951
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-proxy-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2946793056299356999.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-proxy-9.3.11.v20160721.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/zookeeper-3.4.6.jar with timestamp 1516968746950
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/zookeeper-3.4.6.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8288676547946305263.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/zookeeper-3.4.6.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/bcprov-jdk15on-1.51.jar with timestamp 1516968746939
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/bcprov-jdk15on-1.51.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1526685308687029879.tmp
18/01/26 13:12:29 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/bcprov-jdk15on-1.51.jar to class loader
18/01/26 13:12:29 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javax.inject-2.4.0-b34.jar with timestamp 1516968746930
18/01/26 13:12:29 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javax.inject-2.4.0-b34.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2539870546339400704.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javax.inject-2.4.0-b34.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/minlog-1.3.0.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/minlog-1.3.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1087511245375879616.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/minlog-1.3.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scalap-2.11.8.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scalap-2.11.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8375706502355395666.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scalap-2.11.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-client-9.3.11.v20160721.jar with timestamp 1516968746932
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-client-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4122488095714558474.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-client-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/ST4-4.0.4.jar with timestamp 1516968746929
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/ST4-4.0.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1492184680642857827.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/ST4-4.0.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hk2-api-2.4.0-b34.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hk2-api-2.4.0-b34.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4038026814056758509.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hk2-api-2.4.0-b34.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-cli-1.2.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-cli-1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1893699060003249994.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-cli-1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/antlr-2.7.7.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/antlr-2.7.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1606164122622800399.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/antlr-2.7.7.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-annotations-2.6.5.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-annotations-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8633725291392129629.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-annotations-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/shapeless_2.11-2.3.2.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/shapeless_2.11-2.3.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2031318658372742944.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/shapeless_2.11-2.3.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-io-2.4.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-io-2.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6804608534343514238.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-io-2.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8238007702998854314.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-tags_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-compress-1.4.1.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-compress-1.4.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5184351146408102757.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-compress-1.4.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-mapper-asl-1.9.13.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4421844767131661183.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-mapper-asl-1.9.13.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/datanucleus-core-3.2.10.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/datanucleus-core-3.2.10.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8526474850402429164.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/datanucleus-core-3.2.10.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scala-library-2.11.8.jar with timestamp 1516968746929
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scala-library-2.11.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8101768080504344620.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scala-library-2.11.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/mx4j-3.0.2.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/mx4j-3.0.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4106387686884360613.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/mx4j-3.0.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1516968746930
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jcl-over-slf4j-1.7.16.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4792688144947764542.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jcl-over-slf4j-1.7.16.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/eigenbase-properties-1.1.5.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/eigenbase-properties-1.1.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2944854330135889457.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/eigenbase-properties-1.1.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5781871120224284400.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-examples_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-core-2.6.5.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-core-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2458610238470867652.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-core-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746941
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp114163664148977329.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-core_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/guice-3.0.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/guice-3.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8592250114279944006.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/guice-3.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/flume-ng-auth-1.6.0.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/flume-ng-auth-1.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6572396068593661643.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/flume-ng-auth-1.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/curator-client-2.6.0.jar with timestamp 1516968746941
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/curator-client-2.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp9028957210903134693.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/curator-client-2.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-column-1.8.2.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-column-1.8.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3810853838926404124.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-column-1.8.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/RoaringBitmap-0.5.11.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/RoaringBitmap-0.5.11.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5187201068349690500.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/RoaringBitmap-0.5.11.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/kafka-clients-0.10.0.1.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/kafka-clients-0.10.0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4344228486340763411.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/kafka-clients-0.10.0.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-client-2.6.5.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-client-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3652242440403141700.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-yarn-client-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-security-9.3.11.v20160721.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-security-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3163096743344896325.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-security-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-webapp-9.3.11.v20160721.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-webapp-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5490377733353980881.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-webapp-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hk2-utils-2.4.0-b34.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hk2-utils-2.4.0-b34.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8060968351909892527.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hk2-utils-2.4.0-b34.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3424374645845937589.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-compiler-3.0.7.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-compiler-3.0.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6519632756775181509.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-compiler-3.0.7.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-plus-9.3.11.v20160721.jar with timestamp 1516968746941
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-plus-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3927011688312240434.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-plus-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-util-6.1.26.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-util-6.1.26.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6328185478141564101.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-util-6.1.26.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/avro-mapred-1.7.7-hadoop2.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/avro-mapred-1.7.7-hadoop2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1121076178671333775.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/avro-mapred-1.7.7-hadoop2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4679691930988592926.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/machinist_2.11-0.6.1.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/machinist_2.11-0.6.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp9086076885934144514.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/machinist_2.11-0.6.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/univocity-parsers-2.2.1.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/univocity-parsers-2.2.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4610209781223296523.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/univocity-parsers-2.2.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-lang-2.6.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-lang-2.6.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5237905442065160290.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-lang-2.6.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/metrics-graphite-3.1.2.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/metrics-graphite-3.1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4816994909261951081.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/metrics-graphite-3.1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spire-macros_2.11-0.13.0.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spire-macros_2.11-0.13.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3188912917319526189.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spire-macros_2.11-0.13.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javax.inject-1.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javax.inject-1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2170417505057240300.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javax.inject-1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/calcite-avatica-1.2.0-incubating.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/calcite-avatica-1.2.0-incubating.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3154004342170744301.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/calcite-avatica-1.2.0-incubating.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javolution-5.5.1.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javolution-5.5.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2989121195354554108.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javolution-5.5.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/flume-ng-configuration-1.6.0.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/flume-ng-configuration-1.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5487685578341173776.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/flume-ng-configuration-1.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jettison-1.1.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jettison-1.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3236666473514097598.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jettison-1.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5514950644368146765.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-graphx_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/snappy-0.2.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/snappy-0.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1679667407544306666.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/snappy-0.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-6.1.26.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-6.1.26.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6709340734958160664.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-6.1.26.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/joda-time-2.9.3.jar with timestamp 1516968746951
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/joda-time-2.9.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5491722470105512899.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/joda-time-2.9.3.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7476912133037640158.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-module-scala_2.11-2.6.5.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-module-scala_2.11-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6157956619146699683.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-module-scala_2.11-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/compress-lzf-1.0.3.jar with timestamp 1516968746930
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/compress-lzf-1.0.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5683817392026732017.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/compress-lzf-1.0.3.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/macro-compat_2.11-1.1.1.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/macro-compat_2.11-1.1.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2018692956162456774.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/macro-compat_2.11-1.1.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/log4j-1.2.17.jar with timestamp 1516968746956
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/log4j-1.2.17.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4827818720763287148.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/log4j-1.2.17.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/api-util-1.0.0-M20.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/api-util-1.0.0-M20.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4913201257224022928.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/api-util-1.0.0-M20.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-common-1.8.2.jar with timestamp 1516968746939
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-common-1.8.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3933139000974056234.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-common-1.8.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-jaxrs-1.9.13.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-jaxrs-1.9.13.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7510564803857062344.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-jaxrs-1.9.13.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jets3t-0.9.3.jar with timestamp 1516968746930
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jets3t-0.9.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp57852886204624586.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jets3t-0.9.3.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/objenesis-2.1.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/objenesis-2.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3812134698363210419.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/objenesis-2.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/validation-api-1.1.0.Final.jar with timestamp 1516968746930
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/validation-api-1.1.0.Final.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp273438265491868741.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/validation-api-1.1.0.Final.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/xmlenc-0.52.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/xmlenc-0.52.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6709302029563134458.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/xmlenc-0.52.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/breeze_2.11-0.13.2.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/breeze_2.11-0.13.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8420519040083322762.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/breeze_2.11-0.13.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/datanucleus-api-jdo-3.2.6.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/datanucleus-api-jdo-3.2.6.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp875141768644455949.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/datanucleus-api-jdo-3.2.6.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-beanutils-core-1.8.0.jar with timestamp 1516968746932
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-beanutils-core-1.8.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8549964462894873416.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-beanutils-core-1.8.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/pmml-schema-1.2.15.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/pmml-schema-1.2.15.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1965031679940210160.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/pmml-schema-1.2.15.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-api-2.6.5.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-api-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6922212091300911408.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-yarn-api-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/pyrolite-4.13.jar with timestamp 1516968746933
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/pyrolite-4.13.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5447989745436975406.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/pyrolite-4.13.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3881398842914767409.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-hive_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jta-1.1.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jta-1.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3260590162849680325.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jta-1.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/xz-1.0.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/xz-1.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5465223056316653709.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/xz-1.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-servlets-9.3.11.v20160721.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-servlets-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8608065369764326303.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-servlets-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/apache-log4j-extras-1.2.17.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/apache-log4j-extras-1.2.17.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7736499566880755148.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/apache-log4j-extras-1.2.17.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/xbean-asm5-shaded-4.4.jar with timestamp 1516968746951
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/xbean-asm5-shaded-4.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp650036586648970569.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/xbean-asm5-shaded-4.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/apacheds-i18n-2.0.0-M15.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7462348786985380621.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/apacheds-i18n-2.0.0-M15.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-common-2.6.5.jar with timestamp 1516968746933
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-common-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5803783830304981462.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-common-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/py4j-0.10.4.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/py4j-0.10.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3794073985349009367.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/py4j-0.10.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-io-9.3.11.v20160721.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-io-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5478960152936865165.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-io-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-common-2.22.2.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-common-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp359380463824653203.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-common-2.22.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746929
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6607099056109837105.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/avro-ipc-1.7.7.jar with timestamp 1516968746929
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/avro-ipc-1.7.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1476587393390683770.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/avro-ipc-1.7.7.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/kryo-shaded-3.0.3.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/kryo-shaded-3.0.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8527249853286850550.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/kryo-shaded-3.0.3.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hive-metastore-1.2.1.spark2.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hive-metastore-1.2.1.spark2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp773684987321910123.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hive-metastore-1.2.1.spark2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/metrics-core-2.2.0.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/metrics-core-2.2.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4709729162111648938.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/metrics-core-2.2.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/leveldbjni-all-1.8.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/leveldbjni-all-1.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6686565368971296309.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/leveldbjni-all-1.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/snappy-java-1.1.2.6.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/snappy-java-1.1.2.6.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4090643424993532113.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/snappy-java-1.1.2.6.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/kafka_2.11-0.8.2.1.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/kafka_2.11-0.8.2.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8127583707911956087.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/kafka_2.11-0.8.2.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jsr305-1.3.9.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jsr305-1.3.9.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8714291116517276693.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jsr305-1.3.9.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-auth-2.6.5.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-auth-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3680585821074572335.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-auth-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-app-2.6.5.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-app-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6251302704485805509.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-mapreduce-client-app-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/derby-10.12.1.1.jar with timestamp 1516968746951
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/derby-10.12.1.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp201808414461789234.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/derby-10.12.1.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-xml-9.3.11.v20160721.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-xml-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5408989630680784722.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-xml-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/avro-1.7.7.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/avro-1.7.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6375723282782038004.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/avro-1.7.7.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-server-9.3.11.v20160721.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-server-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4863068191271448695.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-server-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-net-3.1.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-net-3.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2376539277640257537.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-net-3.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-hadoop-bundle-1.6.0.jar with timestamp 1516968746940
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-hadoop-bundle-1.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4666687069856566166.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-hadoop-bundle-1.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-hadoop-1.8.2.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-hadoop-1.8.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp139357374033012252.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-hadoop-1.8.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-logging-1.2.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-logging-1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4079435586318957265.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-logging-1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-codec-1.10.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-codec-1.10.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4816000366108711893.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-codec-1.10.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scala-reflect-2.11.8.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scala-reflect-2.11.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1839266046742626354.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scala-reflect-2.11.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-crypto-1.0.0.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-crypto-1.0.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8328007585773806497.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-crypto-1.0.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/json4s-core_2.11-3.2.11.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/json4s-core_2.11-3.2.11.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2110698545597556492.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/json4s-core_2.11-3.2.11.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-core-asl-1.9.13.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-core-asl-1.9.13.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp170818443590685766.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-core-asl-1.9.13.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-math3-3.4.1.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-math3-3.4.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1294055696849292499.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-math3-3.4.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-beanutils-1.7.0.jar with timestamp 1516968746951
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-beanutils-1.7.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8857410408100820204.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-beanutils-1.7.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/curator-framework-2.6.0.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/curator-framework-2.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6671321692782518390.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/curator-framework-2.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/guava-14.0.1.jar with timestamp 1516968746939
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/guava-14.0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5124036945121261938.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/guava-14.0.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746932
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3235910971828985649.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-util-9.3.11.v20160721.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-util-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8140988426398633859.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-util-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-core-2.6.5.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-core-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8706295746543703550.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-mapreduce-client-core-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jline-0.9.94.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jline-0.9.94.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8399761154446070835.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jline-0.9.94.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3785766504447939485.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-databind-2.6.5.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-databind-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3046213329962095150.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-databind-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-configuration-1.6.jar with timestamp 1516968746929
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-configuration-1.6.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1788476098555460982.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-configuration-1.6.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/core-1.1.2.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/core-1.1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp940062772513130094.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/core-1.1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/mail-1.4.7.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/mail-1.4.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7116703384128575668.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/mail-1.4.7.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hive-exec-1.2.1.spark2.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hive-exec-1.2.1.spark2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1611248353638580345.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hive-exec-1.2.1.spark2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/antlr-runtime-3.4.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/antlr-runtime-3.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2748880309181945604.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/antlr-runtime-3.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/ivy-2.4.0.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/ivy-2.4.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3600968149478533523.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/ivy-2.4.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-digester-1.8.jar with timestamp 1516968746939
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-digester-1.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp268386480915935484.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-digester-1.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/calcite-core-1.2.0-incubating.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/calcite-core-1.2.0-incubating.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp943988642390448592.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/calcite-core-1.2.0-incubating.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scala-xml_2.11-1.0.4.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scala-xml_2.11-1.0.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2050030153855462158.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scala-xml_2.11-1.0.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-media-jaxb-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5816640168761959679.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-media-jaxb-2.22.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/aopalliance-repackaged-2.4.0-b34.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1641657302226685985.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/aopalliance-repackaged-2.4.0-b34.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javax.annotation-api-1.2.jar with timestamp 1516968746928
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javax.annotation-api-1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8174813179909299257.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javax.annotation-api-1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/java-xmlbuilder-1.0.jar with timestamp 1516968746933
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/java-xmlbuilder-1.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3205510607657959066.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/java-xmlbuilder-1.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/datanucleus-rdbms-3.2.9.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/datanucleus-rdbms-3.2.9.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5136412913674268994.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/datanucleus-rdbms-3.2.9.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/servlet-api-2.5-20110124.jar with timestamp 1516968746932
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/servlet-api-2.5-20110124.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3999420384156081652.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/servlet-api-2.5-20110124.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/httpcore-4.4.4.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/httpcore-4.4.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4231687121064874392.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/httpcore-4.4.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-server-2.22.2.jar with timestamp 1516968746951
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-server-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4026055975740162877.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-server-2.22.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/json4s-jackson_2.11-3.2.11.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/json4s-jackson_2.11-3.2.11.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3885575269985128639.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/json4s-jackson_2.11-3.2.11.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/api-asn1-api-1.0.0-M20.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2714156707942116899.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/api-asn1-api-1.0.0-M20.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-xc-1.9.13.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-xc-1.9.13.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2695781818055699737.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-xc-1.9.13.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hk2-locator-2.4.0-b34.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hk2-locator-2.4.0-b34.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7936534224164513518.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hk2-locator-2.4.0-b34.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-common-2.6.5.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-common-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4874182507207471748.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-yarn-common-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/zkclient-0.3.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/zkclient-0.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6877016872945001371.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/zkclient-0.3.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jodd-core-3.5.2.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jodd-core-3.5.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4770244217977271871.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jodd-core-3.5.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/velocity-1.7.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/velocity-1.7.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp541579545868289738.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/velocity-1.7.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javax.ws.rs-api-2.0.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7420681131680435514.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javax.ws.rs-api-2.0.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scopt_2.11-3.3.0.jar with timestamp 1516968746931
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scopt_2.11-3.3.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5066004704324594652.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scopt_2.11-3.3.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-annotations-2.6.5.jar with timestamp 1516968746949
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-annotations-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3245164803592970739.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-annotations-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/apacheds-kerberos-codec-2.0.0-M15.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8962789065056756827.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/apacheds-kerberos-codec-2.0.0-M15.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/cglib-2.2.1-v20090111.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/cglib-2.2.1-v20090111.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6005920099704375083.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/cglib-2.2.1-v20090111.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6300766438424827742.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-mapreduce-client-jobclient-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-container-servlet-core-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7828798731129967295.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-container-servlet-core-2.22.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/xercesImpl-2.9.1.jar with timestamp 1516968746930
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/xercesImpl-2.9.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8677222215120873754.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/xercesImpl-2.9.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jackson-module-paranamer-2.6.5.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jackson-module-paranamer-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8697779124026004703.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jackson-module-paranamer-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/base64-2.3.8.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/base64-2.3.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5973092361439375765.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/base64-2.3.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7663763812749097426.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-sketch_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jaxb-api-2.2.2.jar with timestamp 1516968746941
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jaxb-api-2.2.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6587675525384307423.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jaxb-api-2.2.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/metrics-jvm-3.1.2.jar with timestamp 1516968746942
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/metrics-jvm-3.1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3335126212500201810.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/metrics-jvm-3.1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/protobuf-java-2.5.0.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/protobuf-java-2.5.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8900722810633975583.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/protobuf-java-2.5.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/slf4j-log4j12-1.7.16.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/slf4j-log4j12-1.7.16.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3362586587831762024.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/slf4j-log4j12-1.7.16.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/paranamer-2.6.jar with timestamp 1516968746934
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/paranamer-2.6.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7132399632625907261.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/paranamer-2.6.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-client-2.6.5.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-client-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3933423702412948436.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-client-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-continuation-9.3.11.v20160721.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-continuation-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2213906608320674412.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-continuation-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp533917734332172288.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-mapreduce-client-shuffle-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-dbcp-1.4.jar with timestamp 1516968746931
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-dbcp-1.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6704990287763134610.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-dbcp-1.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1516968746943
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/bonecp-0.8.0.RELEASE.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5911030473111970183.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/bonecp-0.8.0.RELEASE.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/httpclient-4.5.2.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/httpclient-4.5.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4207966087476560383.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/httpclient-4.5.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/lz4-1.3.0.jar with timestamp 1516968746953
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/lz4-1.3.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4654213706143745798.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/lz4-1.3.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javax.servlet-api-3.1.0.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javax.servlet-api-3.1.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4515925110092263912.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javax.servlet-api-3.1.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/xml-apis-1.3.04.jar with timestamp 1516968746952
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/xml-apis-1.3.04.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2255411316754892632.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/xml-apis-1.3.04.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/metrics-json-3.1.2.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/metrics-json-3.1.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5885637412781748645.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/metrics-json-3.1.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/stringtemplate-3.2.1.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/stringtemplate-3.2.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4627734748326178193.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/stringtemplate-3.2.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-common-2.6.5.jar with timestamp 1516968746955
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-mapreduce-client-common-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4789808559752372736.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-mapreduce-client-common-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/flume-ng-sdk-1.6.0.jar with timestamp 1516968746933
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/flume-ng-sdk-1.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7931073635586558268.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/flume-ng-sdk-1.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-server-common-2.6.5.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-yarn-server-common-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp10694281102754807.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-yarn-server-common-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/slf4j-api-1.7.21.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/slf4j-api-1.7.21.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5279753611374708133.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/slf4j-api-1.7.21.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/avro-compiler-1.7.3.jar with timestamp 1516968746947
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/avro-compiler-1.7.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5383627610633020323.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/avro-compiler-1.7.3.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-encoding-1.8.2.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-encoding-1.8.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5597270179813619655.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-encoding-1.8.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-jackson-1.8.2.jar with timestamp 1516968746954
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-jackson-1.8.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4248522933083286036.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-jackson-1.8.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/htrace-core-3.0.4.jar with timestamp 1516968746928
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/htrace-core-3.0.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4876381322744313074.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/htrace-core-3.0.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/flume-ng-core-1.6.0.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/flume-ng-core-1.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2042551828786082328.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/flume-ng-core-1.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-http-9.3.11.v20160721.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-http-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4747671218267617124.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-http-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/hadoop-hdfs-2.6.5.jar with timestamp 1516968746933
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/hadoop-hdfs-2.6.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp1503040093520309383.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/hadoop-hdfs-2.6.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-pool-1.5.4.jar with timestamp 1516968746937
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-pool-1.5.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp5584845769702180229.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-pool-1.5.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/stax-api-1.0-2.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/stax-api-1.0-2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3050685915270521184.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/stax-api-1.0-2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6834607659912083892.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-network-common_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-container-servlet-2.22.2.jar with timestamp 1516968746932
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-container-servlet-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4059485094058094291.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-container-servlet-2.22.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/parquet-format-2.3.1.jar with timestamp 1516968746948
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/parquet-format-2.3.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2852950071342367753.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/parquet-format-2.3.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar with timestamp 1516968746929
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3229238319248429128.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spark-streaming_2.11-2.2.2-SNAPSHOT.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/netty-all-4.0.43.Final.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/netty-all-4.0.43.Final.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp2358253317274420926.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/netty-all-4.0.43.Final.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jersey-guava-2.22.2.jar with timestamp 1516968746944
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jersey-guava-2.22.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6678011937162400138.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jersey-guava-2.22.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/chill-java-0.8.0.jar with timestamp 1516968746946
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/chill-java-0.8.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4066636641894463545.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/chill-java-0.8.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/spire_2.11-0.13.0.jar with timestamp 1516968746936
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/spire_2.11-0.13.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7038988958008737997.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/spire_2.11-0.13.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/pmml-model-1.2.15.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/pmml-model-1.2.15.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp306812448791355380.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/pmml-model-1.2.15.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/curator-recipes-2.6.0.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/curator-recipes-2.6.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7214169707213857699.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/curator-recipes-2.6.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/jetty-servlet-9.3.11.v20160721.jar with timestamp 1516968746931
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/jetty-servlet-9.3.11.v20160721.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp803676592745494126.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/jetty-servlet-9.3.11.v20160721.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/breeze-macros_2.11-0.13.2.jar with timestamp 1516968746930
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/breeze-macros_2.11-0.13.2.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp4004318311935056959.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/breeze-macros_2.11-0.13.2.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/javassist-3.18.1-GA.jar with timestamp 1516968746933
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/javassist-3.18.1-GA.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp6738928641605224107.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/javassist-3.18.1-GA.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1516968746950
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/scala-parser-combinators_2.11-1.0.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp858376956502050238.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/scala-parser-combinators_2.11-1.0.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/activation-1.1.1.jar with timestamp 1516968746945
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/activation-1.1.1.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7954308657176634870.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/activation-1.1.1.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/oro-2.0.8.jar with timestamp 1516968746951
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/oro-2.0.8.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp9123600865691144351.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/oro-2.0.8.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/commons-lang3-3.5.jar with timestamp 1516968746941
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/commons-lang3-3.5.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp3964670746417716435.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/commons-lang3-3.5.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/aopalliance-1.0.jar with timestamp 1516968746941
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/aopalliance-1.0.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7054446609625408016.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/aopalliance-1.0.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/mina-core-2.0.4.jar with timestamp 1516968746938
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/mina-core-2.0.4.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp7079942959826283499.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/mina-core-2.0.4.jar to class loader
18/01/26 13:12:30 INFO Executor: Fetching spark://138.96.200.169:46501/jars/libfb303-0.9.3.jar with timestamp 1516968746935
18/01/26 13:12:30 INFO Utils: Fetching spark://138.96.200.169:46501/jars/libfb303-0.9.3.jar to /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/fetchFileTemp8496698567201177548.tmp
18/01/26 13:12:30 INFO Executor: Adding file:/tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e/userFiles-085032fc-a736-4774-b922-10000ca5d808/libfb303-0.9.3.jar to class loader
18/01/26 13:12:30 INFO CodeGenerator: Code generated in 14.400687 ms
18/01/26 13:12:30 INFO MemoryStore: Block rdd_4_3 stored as values in memory (estimated size 136.0 B, free 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block rdd_4_1 stored as values in memory (estimated size 136.0 B, free 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block rdd_4_0 stored as values in memory (estimated size 184.0 B, free 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block rdd_4_2 stored as values in memory (estimated size 160.0 B, free 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added rdd_4_3 in memory on 138.96.200.169:42291 (size: 136.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added rdd_4_0 in memory on 138.96.200.169:42291 (size: 184.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added rdd_4_2 in memory on 138.96.200.169:42291 (size: 160.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added rdd_4_1 in memory on 138.96.200.169:42291 (size: 136.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 67006 bytes result sent to driver
18/01/26 13:12:30 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 67006 bytes result sent to driver
18/01/26 13:12:30 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 67006 bytes result sent to driver
18/01/26 13:12:30 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 67006 bytes result sent to driver
18/01/26 13:12:30 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 829 ms on localhost (executor driver) (1/4)
18/01/26 13:12:30 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 832 ms on localhost (executor driver) (2/4)
18/01/26 13:12:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 842 ms on localhost (executor driver) (3/4)
18/01/26 13:12:30 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 832 ms on localhost (executor driver) (4/4)
18/01/26 13:12:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/01/26 13:12:30 INFO DAGScheduler: ResultStage 0 (treeAggregate at LogisticRegression.scala:517) finished in 0.855 s
18/01/26 13:12:30 INFO DAGScheduler: Job 0 finished: treeAggregate at LogisticRegression.scala:517, took 1.151765 s
18/01/26 13:12:30 INFO Instrumentation: LogisticRegression-logreg_ab3e78168e03-1079214960-1: {"numClasses":2}
18/01/26 13:12:30 INFO Instrumentation: LogisticRegression-logreg_ab3e78168e03-1079214960-1: {"numFeatures":1000}
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.9 KB, free 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 173.0 B, free 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 138.96.200.169:42291 (size: 173.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO SparkContext: Created broadcast 1 from broadcast at LogisticRegression.scala:600
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.9 KB, free 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 182.0 B, free 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 138.96.200.169:42291 (size: 182.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO SparkContext: Created broadcast 2 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:30 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:30 INFO DAGScheduler: Got job 1 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:30 INFO DAGScheduler: Final stage: ResultStage 1 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:30 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:30 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||   id: 1,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 1,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 6,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     {id: 6,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[6] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:30 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:30 WARN Simulator: || SIMULATION || Starting for job = 1
18/01/26 13:12:30 WARN Simulator: || SIMULATION || submitStage(ResultStage 1)
18/01/26 13:12:30 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:30 WARN Simulator: || SIMULATION || Statistics for 1:
18/01/26 13:12:30 WARN Simulator: || SIMULATION || hits = 1
18/01/26 13:12:30 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:30 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:30 WARN Simulator: || SIMULATION || narrowDependencies = 6
18/01/26 13:12:30 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:30 WARN Simulator: || SIMULATION || Finished for job = 1
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.4 KB, free 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KB, free 366.3 MB)
18/01/26 13:12:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 138.96.200.169:42291 (size: 4.9 KB, free: 366.3 MB)
18/01/26 13:12:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:30 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks
18/01/26 13:12:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:30 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:30 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:30 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:30 INFO Executor: Running task 2.0 in stage 1.0 (TID 6)
18/01/26 13:12:30 INFO Executor: Running task 1.0 in stage 1.0 (TID 5)
18/01/26 13:12:30 INFO Executor: Running task 3.0 in stage 1.0 (TID 7)
18/01/26 13:12:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 4)
18/01/26 13:12:30 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:30 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:30 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:30 INFO Executor: Finished task 2.0 in stage 1.0 (TID 6). 10638 bytes result sent to driver
18/01/26 13:12:30 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:30 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 6) in 15 ms on localhost (executor driver) (1/4)
18/01/26 13:12:30 INFO Executor: Finished task 1.0 in stage 1.0 (TID 5). 10638 bytes result sent to driver
18/01/26 13:12:30 INFO Executor: Finished task 3.0 in stage 1.0 (TID 7). 10638 bytes result sent to driver
18/01/26 13:12:30 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 5) in 16 ms on localhost (executor driver) (2/4)
18/01/26 13:12:30 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 7) in 16 ms on localhost (executor driver) (3/4)
18/01/26 13:12:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 4). 10638 bytes result sent to driver
18/01/26 13:12:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 4) in 19 ms on localhost (executor driver) (4/4)
18/01/26 13:12:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/01/26 13:12:30 INFO DAGScheduler: ResultStage 1 (treeAggregate at LogisticRegression.scala:1894) finished in 0.021 s
18/01/26 13:12:30 INFO DAGScheduler: Job 1 finished: treeAggregate at LogisticRegression.scala:1894, took 0.033527 s
18/01/26 13:12:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
18/01/26 13:12:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
18/01/26 13:12:30 INFO TorrentBroadcast: Destroying Broadcast(2) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 138.96.200.169:42291 in memory (size: 182.0 B, free: 366.3 MB)
18/01/26 13:12:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.9 KB, free 366.3 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 244.0 B, free 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 138.96.200.169:42291 (size: 244.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 4 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 2 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 2 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 7,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 7,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 2
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 2)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 2:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 2
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 7
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 2
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 10.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 138.96.200.169:42291 (size: 5.0 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 9, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 10, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 11, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 2.0 (TID 9)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 2.0 (TID 8)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 2.0 (TID 10)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 2.0 (TID 11)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 2.0 (TID 8). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 2.0 (TID 10). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 2.0 (TID 11). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 2.0 (TID 9). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 9 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 10) in 9 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 9) in 9 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 11) in 9 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 2 (treeAggregate at LogisticRegression.scala:1894) finished in 0.011 s
18/01/26 13:12:31 INFO DAGScheduler: Job 2 finished: treeAggregate at LogisticRegression.scala:1894, took 0.018571 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(4) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 138.96.200.169:42291 in memory (size: 244.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.206
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.271355 (rel: 0.609) 0.360241
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 276.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 138.96.200.169:42291 (size: 276.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 6 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 3 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 3 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 8,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 8,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 3
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 3)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 3:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 3
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 8
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 3
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.4 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.0 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 138.96.200.169:42291 (size: 5.0 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 3.0 (TID 14)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 3.0 (TID 13)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 3.0 (TID 15)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 12)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 3.0 (TID 14). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 14) in 6 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 3.0 (TID 15). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 3.0 (TID 12). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 15) in 11 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 12) in 11 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 3.0 (TID 13). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 13) in 12 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 3 (treeAggregate at LogisticRegression.scala:1894) finished in 0.013 s
18/01/26 13:12:31 INFO DAGScheduler: Job 3 finished: treeAggregate at LogisticRegression.scala:1894, took 0.021553 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(6) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 138.96.200.169:42291 in memory (size: 276.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.115428 (rel: 0.575) 0.149415
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 8 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 4 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 4 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 9,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 9,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 4
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 4)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 4:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 4
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 9
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 4
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.8 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 138.96.200.169:42291 in memory (size: 4.6 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 138.96.200.169:42291 in memory (size: 4.9 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 138.96.200.169:42291 in memory (size: 5.0 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 138.96.200.169:42291 (size: 5.1 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 138.96.200.169:42291 in memory (size: 5.0 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 18, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 19, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 4.0 (TID 16)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 4.0 (TID 19)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 4.0 (TID 17)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 4.0 (TID 18)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 4.0 (TID 18). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 4.0 (TID 17). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 4.0 (TID 19). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 11 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 18) in 11 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 11 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 4.0 (TID 16). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 14 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 4 (treeAggregate at LogisticRegression.scala:1894) finished in 0.014 s
18/01/26 13:12:31 INFO DAGScheduler: Job 4 finished: treeAggregate at LogisticRegression.scala:1894, took 0.041703 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(8) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0559292 (rel: 0.515) 0.0693937
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.9 KB, free 366.3 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 10 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 5 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 5 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 5,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 5,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 10,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 10,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 5
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 5)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 5:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 5
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 10
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 5
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 12.3 KB, free 366.3 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.1 KB, free 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 138.96.200.169:42291 (size: 5.1 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 22, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 23, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 20)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 5.0 (TID 22)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 5.0 (TID 23)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 5.0 (TID 21)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 5.0 (TID 21). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 20). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 5.0 (TID 23). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 21) in 7 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 23) in 7 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 20) in 8 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 5.0 (TID 22). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 22) in 12 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 5 (treeAggregate at LogisticRegression.scala:1894) finished in 0.012 s
18/01/26 13:12:31 INFO DAGScheduler: Job 5 finished: treeAggregate at LogisticRegression.scala:1894, took 0.021718 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(10) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0293811 (rel: 0.475) 0.0328346
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.9 KB, free 366.3 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.3 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 12 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 6 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 6 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[11] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 6,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 6,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 11,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 11,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[11] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 6
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 6)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 6:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 6
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 6
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.8 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.3 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 138.96.200.169:42291 (size: 5.3 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 6 (MapPartitionsRDD[11] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 6.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 26, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 27, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 6.0 (TID 27)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 6.0 (TID 26)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 6.0 (TID 24)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 6.0 (TID 25)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 6.0 (TID 26). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 26) in 6 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 6.0 (TID 24). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 24) in 8 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 6.0 (TID 27). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 6.0 (TID 25). 10595 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 27) in 9 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 25) in 9 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 6 (treeAggregate at LogisticRegression.scala:1894) finished in 0.010 s
18/01/26 13:12:31 INFO DAGScheduler: Job 6 finished: treeAggregate at LogisticRegression.scala:1894, took 0.018372 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(12) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0180431 (rel: 0.386) 0.0155588
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 287.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 138.96.200.169:42291 (size: 287.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 14 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 7 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 7 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[12] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 7,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 7,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 12,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 12,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[12] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 7
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 7)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 7:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 7
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 12
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 7
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 13.3 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.2 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 138.96.200.169:42291 (size: 5.2 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 7 (MapPartitionsRDD[12] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 7.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 29, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 30, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 31, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 7.0 (TID 30)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 7.0 (TID 29)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 7.0 (TID 28)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 7.0 (TID 31)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 7.0 (TID 30). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 7.0 (TID 31). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 31) in 7 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 30) in 7 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 28). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 7.0 (TID 29). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 28) in 12 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 29) in 12 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 7 (treeAggregate at LogisticRegression.scala:1894) finished in 0.013 s
18/01/26 13:12:31 INFO DAGScheduler: Job 7 finished: treeAggregate at LogisticRegression.scala:1894, took 0.021492 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(14) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 138.96.200.169:42291 in memory (size: 287.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0132568 (rel: 0.265) 0.00697143
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 16 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 8 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 8 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[13] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 8,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 8,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 13,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 13,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[13] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 8
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 8)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 8:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 8
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 13
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 8
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.7 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.3 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 138.96.200.169:42291 (size: 5.3 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 8 (MapPartitionsRDD[13] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 8.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 33, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 34, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 35, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 8.0 (TID 34)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 8.0 (TID 32)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 8.0 (TID 35)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 8.0 (TID 33)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 8.0 (TID 34). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 34) in 8 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 8.0 (TID 35). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 35) in 9 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 8.0 (TID 32). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 32) in 10 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 8.0 (TID 33). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 33) in 14 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 8 (treeAggregate at LogisticRegression.scala:1894) finished in 0.016 s
18/01/26 13:12:31 INFO DAGScheduler: Job 8 finished: treeAggregate at LogisticRegression.scala:1894, took 0.023872 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(16) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0115421 (rel: 0.129) 0.00282565
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 18 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 9 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 9 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[14] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 9,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 9,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 14,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 14,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[14] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 9
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 9)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 9:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 9
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 14
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 9
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.2 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.3 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 138.96.200.169:42291 (size: 5.3 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[14] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 9.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 38, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 39, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 9.0 (TID 36)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 9.0 (TID 38)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 9.0 (TID 37)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 9.0 (TID 39)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 9.0 (TID 38). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 38) in 9 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 9.0 (TID 37). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 9.0 (TID 39). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 37) in 10 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 39) in 11 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 9.0 (TID 36). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 36) in 13 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 9 (treeAggregate at LogisticRegression.scala:1894) finished in 0.014 s
18/01/26 13:12:31 INFO DAGScheduler: Job 9 finished: treeAggregate at LogisticRegression.scala:1894, took 0.021856 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(18) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0110450 (rel: 0.0431) 0.00109957
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 20 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 10 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 10 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[15] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 10,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 10,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 15,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 15,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[15] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 10
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 10)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 10:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 10
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 15
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 10
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 14.7 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.3 KB, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 138.96.200.169:42291 (size: 5.3 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[15] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 41, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 42, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 43, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 10.0 (TID 40)
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 10.0 (TID 43)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 10.0 (TID 42)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 10.0 (TID 41)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 10.0 (TID 40). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 40) in 9 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 10.0 (TID 41). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 10.0 (TID 43). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 41) in 13 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 10.0 (TID 42). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 43) in 13 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 42) in 14 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 10 (treeAggregate at LogisticRegression.scala:1894) finished in 0.015 s
18/01/26 13:12:31 INFO DAGScheduler: Job 10 finished: treeAggregate at LogisticRegression.scala:1894, took 0.021219 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(20) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0108745 (rel: 0.0154) 0.000812812
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 7.9 KB, free 366.2 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 284.0 B, free 366.2 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 138.96.200.169:42291 (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 22 from broadcast at LogisticRegression.scala:1881
18/01/26 13:12:31 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
18/01/26 13:12:31 INFO DAGScheduler: Got job 11 (treeAggregate at LogisticRegression.scala:1894) with 4 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 11 (treeAggregate at LogisticRegression.scala:1894)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[16] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 11,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 11,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 16,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 16,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[16] at treeAggregate at LogisticRegression.scala:1894,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 3,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 2,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[1] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 0,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[0] at rdd at LogisticRegression.scala:495,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 4,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 11)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 11:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 16
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 11
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.1 KB, free 366.1 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.4 KB, free 366.1 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 138.96.200.169:42291 (size: 5.4 KB, free: 366.3 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 11 (MapPartitionsRDD[16] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 11.0 with 4 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 5163 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 45, localhost, executor driver, partition 1, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 46, localhost, executor driver, partition 2, PROCESS_LOCAL, 5139 bytes)
18/01/26 13:12:31 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 47, localhost, executor driver, partition 3, PROCESS_LOCAL, 5115 bytes)
18/01/26 13:12:31 INFO Executor: Running task 2.0 in stage 11.0 (TID 46)
18/01/26 13:12:31 INFO Executor: Running task 1.0 in stage 11.0 (TID 45)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 11.0 (TID 44)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_0 locally
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_2 locally
18/01/26 13:12:31 INFO Executor: Running task 3.0 in stage 11.0 (TID 47)
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 11.0 (TID 44). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 44) in 7 ms on localhost (executor driver) (1/4)
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_1 locally
18/01/26 13:12:31 INFO Executor: Finished task 2.0 in stage 11.0 (TID 46). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO BlockManager: Found block rdd_4_3 locally
18/01/26 13:12:31 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 46) in 9 ms on localhost (executor driver) (2/4)
18/01/26 13:12:31 INFO Executor: Finished task 3.0 in stage 11.0 (TID 47). 10638 bytes result sent to driver
18/01/26 13:12:31 INFO Executor: Finished task 1.0 in stage 11.0 (TID 45). 10681 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 47) in 10 ms on localhost (executor driver) (3/4)
18/01/26 13:12:31 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 45) in 10 ms on localhost (executor driver) (4/4)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 11 (treeAggregate at LogisticRegression.scala:1894) finished in 0.011 s
18/01/26 13:12:31 INFO DAGScheduler: Job 11 finished: treeAggregate at LogisticRegression.scala:1894, took 0.018782 s
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(22) (from destroy at LogisticRegression.scala:1935)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 138.96.200.169:42291 in memory (size: 284.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO LBFGS: Step Size: 1.000
18/01/26 13:12:31 INFO LBFGS: Val and Grad Norm: 0.0106904 (rel: 0.0169) 0.00106124
18/01/26 13:12:31 INFO LBFGS: Converged because max iterations reached
18/01/26 13:12:31 INFO TorrentBroadcast: Destroying Broadcast(1) (from destroy at LogisticRegression.scala:796)
18/01/26 13:12:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 138.96.200.169:42291 in memory (size: 173.0 B, free: 366.3 MB)
18/01/26 13:12:31 INFO MapPartitionsRDD: Removing RDD 4 from persistence list
18/01/26 13:12:31 INFO BlockManager: Removing RDD 4
18/01/26 13:12:31 INFO CodeGenerator: Code generated in 20.471608 ms
18/01/26 13:12:31 INFO Instrumentation: LogisticRegression-logreg_ab3e78168e03-1079214960-1: training finished
18/01/26 13:12:31 INFO PipelineModel$PipelineModelWriter: Path /tmp/spark-logistic-regression-model already exists. It will be overwritten.
18/01/26 13:12:31 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:31 INFO DAGScheduler: Got job 12 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 12 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[23] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 12,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 12,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 23,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 23,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[23] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 22}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 22,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[22] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 12
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 12)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 12:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 17
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 12
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 64.2 KB, free 366.1 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 22.5 KB, free 366.1 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[23] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 5056 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 12.0 (TID 48)
18/01/26 13:12:31 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131231_0012_m_000000_48' to file:/tmp/spark-logistic-regression-model/metadata/_temporary/0/task_20180126131231_0012_m_000000
18/01/26 13:12:31 INFO SparkHadoopMapRedUtil: attempt_20180126131231_0012_m_000000_48: Committed
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 12.0 (TID 48). 794 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 48) in 36 ms on localhost (executor driver) (1/1)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 12 (saveAsTextFile at ReadWrite.scala:278) finished in 0.036 s
18/01/26 13:12:31 INFO DAGScheduler: Job 12 finished: saveAsTextFile at ReadWrite.scala:278, took 0.049102 s
18/01/26 13:12:31 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:31 INFO DAGScheduler: Got job 13 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 13 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[25] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 13,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 13,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 25,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 25,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[25] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 24}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 24,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[24] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 13
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 13)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 13:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 18
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 13
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 64.2 KB, free 366.0 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 22.5 KB, free 366.0 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[25] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 5013 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 13.0 (TID 49)
18/01/26 13:12:31 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131231_0013_m_000000_49' to file:/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata/_temporary/0/task_20180126131231_0013_m_000000
18/01/26 13:12:31 INFO SparkHadoopMapRedUtil: attempt_20180126131231_0013_m_000000_49: Committed
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 13.0 (TID 49). 794 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 49) in 16 ms on localhost (executor driver) (1/1)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 13 (saveAsTextFile at ReadWrite.scala:278) finished in 0.016 s
18/01/26 13:12:31 INFO DAGScheduler: Job 13 finished: saveAsTextFile at ReadWrite.scala:278, took 0.027873 s
18/01/26 13:12:31 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:31 INFO DAGScheduler: Got job 14 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 14 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[27] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 14,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 14,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 27,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 27,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[27] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 26}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 26,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[26] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 14
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 14)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 14:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 19
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 14
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 64.2 KB, free 365.9 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 22.5 KB, free 365.9 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[27] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 50, localhost, executor driver, partition 0, PROCESS_LOCAL, 5057 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 14.0 (TID 50)
18/01/26 13:12:31 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131231_0014_m_000000_50' to file:/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata/_temporary/0/task_20180126131231_0014_m_000000
18/01/26 13:12:31 INFO SparkHadoopMapRedUtil: attempt_20180126131231_0014_m_000000_50: Committed
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 14.0 (TID 50). 794 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 50) in 15 ms on localhost (executor driver) (1/1)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 14 (saveAsTextFile at ReadWrite.scala:278) finished in 0.015 s
18/01/26 13:12:31 INFO DAGScheduler: Job 14 finished: saveAsTextFile at ReadWrite.scala:278, took 0.025855 s
18/01/26 13:12:31 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:31 INFO DAGScheduler: Got job 15 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 15 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:31 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[29] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 15,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 15,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 29,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 29,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[29] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 28}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 28,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[28] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Starting for job = 15
18/01/26 13:12:31 WARN Simulator: || SIMULATION || submitStage(ResultStage 15)
18/01/26 13:12:31 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Statistics for 15:
18/01/26 13:12:31 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:31 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:31 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || narrowDependencies = 20
18/01/26 13:12:31 WARN Simulator: || SIMULATION || shuffleDpendencies = 0
18/01/26 13:12:31 WARN Simulator: || SIMULATION || Finished for job = 15
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 64.2 KB, free 365.8 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 22.6 KB, free 365.8 MB)
18/01/26 13:12:31 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 138.96.200.169:42291 (size: 22.6 KB, free: 366.2 MB)
18/01/26 13:12:31 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[29] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:31 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
18/01/26 13:12:31 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 5299 bytes)
18/01/26 13:12:31 INFO Executor: Running task 0.0 in stage 15.0 (TID 51)
18/01/26 13:12:31 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131231_0015_m_000000_51' to file:/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata/_temporary/0/task_20180126131231_0015_m_000000
18/01/26 13:12:31 INFO SparkHadoopMapRedUtil: attempt_20180126131231_0015_m_000000_51: Committed
18/01/26 13:12:31 INFO Executor: Finished task 0.0 in stage 15.0 (TID 51). 837 bytes result sent to driver
18/01/26 13:12:31 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 51) in 14 ms on localhost (executor driver) (1/1)
18/01/26 13:12:31 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
18/01/26 13:12:31 INFO DAGScheduler: ResultStage 15 (saveAsTextFile at ReadWrite.scala:278) finished in 0.015 s
18/01/26 13:12:31 INFO DAGScheduler: Job 15 finished: saveAsTextFile at ReadWrite.scala:278, took 0.028461 s
18/01/26 13:12:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
18/01/26 13:12:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
18/01/26 13:12:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
18/01/26 13:12:31 INFO CodeGenerator: Code generated in 40.645408 ms
18/01/26 13:12:31 INFO SparkContext: Starting job: parquet at LogisticRegression.scala:1187
18/01/26 13:12:31 INFO DAGScheduler: Registering RDD 32 (parquet at LogisticRegression.scala:1187)
18/01/26 13:12:31 INFO DAGScheduler: Got job 16 (parquet at LogisticRegression.scala:1187) with 1 output partitions
18/01/26 13:12:31 INFO DAGScheduler: Final stage: ResultStage 17 (parquet at LogisticRegression.scala:1187)
18/01/26 13:12:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
18/01/26 13:12:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)
18/01/26 13:12:31 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[32] at parquet at LogisticRegression.scala:1187), which has no missing parents
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   id: 16,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   name: ShuffleMapStage 16,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 32,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   stage type: ShuffleMapStage,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 32,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at parquet at LogisticRegression.scala:1187,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 31,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[31] at parquet at LogisticRegression.scala:1187,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 30}
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     {id: 30,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[30] at parquet at LogisticRegression.scala:1187,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:31 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.9 KB, free 365.8 MB)
18/01/26 13:12:31 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.4 KB, free 365.8 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 138.96.200.169:42291 (size: 3.4 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[32] at parquet at LogisticRegression.scala:1187) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 52, localhost, executor driver, partition 0, PROCESS_LOCAL, 5347 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 16.0 (TID 52)
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 16.0 (TID 52). 1295 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 52) in 32 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ShuffleMapStage 16 (parquet at LogisticRegression.scala:1187) finished in 0.032 s
18/01/26 13:12:32 INFO DAGScheduler: looking for newly runnable stages
18/01/26 13:12:32 INFO DAGScheduler: running: Set()
18/01/26 13:12:32 INFO DAGScheduler: waiting: Set(ResultStage 17)
18/01/26 13:12:32 INFO DAGScheduler: failed: Set()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 17 (ShuffledRowRDD[33] at parquet at LogisticRegression.scala:1187), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 17,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 17,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 33,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 33,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: ShuffledRowRDD[33] at parquet at LogisticRegression.scala:1187,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: shuffle, rddid: 32}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 16
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 17)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 16:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 20
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 16
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 115.2 KB, free 365.7 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 33.4 KB, free 365.7 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 138.96.200.169:42291 (size: 33.4 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (ShuffledRowRDD[33] at parquet at LogisticRegression.scala:1187) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 53, localhost, executor driver, partition 0, ANY, 4726 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 17.0 (TID 53)
18/01/26 13:12:32 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/01/26 13:12:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
18/01/26 13:12:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
18/01/26 13:12:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
18/01/26 13:12:32 INFO CodecConfig: Compression: SNAPPY
18/01/26 13:12:32 INFO CodecConfig: Compression: SNAPPY
18/01/26 13:12:32 INFO ParquetOutputFormat: Parquet block size to 134217728
18/01/26 13:12:32 INFO ParquetOutputFormat: Parquet page size to 1048576
18/01/26 13:12:32 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
18/01/26 13:12:32 INFO ParquetOutputFormat: Dictionary is on
18/01/26 13:12:32 INFO ParquetOutputFormat: Validation is off
18/01/26 13:12:32 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
18/01/26 13:12:32 INFO ParquetOutputFormat: Maximum row group padding size is 0 bytes
18/01/26 13:12:32 INFO ParquetOutputFormat: Page size checking is: estimated
18/01/26 13:12:32 INFO ParquetOutputFormat: Min row count for page size check is: 100
18/01/26 13:12:32 INFO ParquetOutputFormat: Max row count for page size check is: 10000
18/01/26 13:12:32 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "numClasses",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "numFeatures",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "interceptVector",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "coefficientMatrix",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.MatrixUDT",
      "pyClass" : "pyspark.ml.linalg.MatrixUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "numRows",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "numCols",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "colPtrs",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "rowIndices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "isTransposed",
          "type" : "boolean",
          "nullable" : false,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "isMultinomial",
    "type" : "boolean",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 numClasses;
  required int32 numFeatures;
  optional group interceptVector {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
  optional group coefficientMatrix {
    required int32 type (INT_8);
    required int32 numRows;
    required int32 numCols;
    optional group colPtrs (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group rowIndices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
    required boolean isTransposed;
  }
  required boolean isMultinomial;
}

       
18/01/26 13:12:32 INFO ContextCleaner: Cleaned accumulator 410
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 138.96.200.169:42291 in memory (size: 5.3 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 138.96.200.169:42291 in memory (size: 5.3 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 138.96.200.169:42291 in memory (size: 5.4 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 138.96.200.169:42291 in memory (size: 5.3 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 138.96.200.169:42291 in memory (size: 3.4 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 138.96.200.169:42291 in memory (size: 5.3 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 138.96.200.169:42291 in memory (size: 5.1 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 138.96.200.169:42291 in memory (size: 22.6 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 138.96.200.169:42291 in memory (size: 5.1 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 138.96.200.169:42291 in memory (size: 5.2 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.3 MB)
18/01/26 13:12:32 INFO CodecPool: Got brand-new compressor [.snappy]
18/01/26 13:12:32 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 131512
18/01/26 13:12:32 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131232_0017_m_000000_0' to file:/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/data/_temporary/0/task_20180126131232_0017_m_000000
18/01/26 13:12:32 INFO SparkHadoopMapRedUtil: attempt_20180126131232_0017_m_000000_0: Committed
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 17.0 (TID 53). 1740 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 53) in 315 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 17 (parquet at LogisticRegression.scala:1187) finished in 0.315 s
18/01/26 13:12:32 INFO DAGScheduler: Job 16 finished: parquet at LogisticRegression.scala:1187, took 0.385006 s
18/01/26 13:12:32 INFO FileFormatWriter: Job null committed.
18/01/26 13:12:32 INFO Pipeline$PipelineWriter: Path /tmp/unfit-lr-model already exists. It will be overwritten.
18/01/26 13:12:32 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:32 INFO DAGScheduler: Got job 17 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 18 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 18,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 18,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 37,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 37,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[37] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 36}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 36,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[36] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 17
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 18)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 17:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 21
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 17
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 64.2 KB, free 366.1 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 22.5 KB, free 366.1 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 54, localhost, executor driver, partition 0, PROCESS_LOCAL, 5051 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 18.0 (TID 54)
18/01/26 13:12:32 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131232_0018_m_000000_54' to file:/tmp/unfit-lr-model/metadata/_temporary/0/task_20180126131232_0018_m_000000
18/01/26 13:12:32 INFO SparkHadoopMapRedUtil: attempt_20180126131232_0018_m_000000_54: Committed
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 18.0 (TID 54). 794 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 54) in 10 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 18 (saveAsTextFile at ReadWrite.scala:278) finished in 0.011 s
18/01/26 13:12:32 INFO DAGScheduler: Job 17 finished: saveAsTextFile at ReadWrite.scala:278, took 0.018622 s
18/01/26 13:12:32 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:32 INFO DAGScheduler: Got job 18 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 19 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 19,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 19,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 39,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 39,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[39] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 38}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 38,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[38] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 18
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 19)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 18:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 22
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 18
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 64.2 KB, free 366.0 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 22.5 KB, free 366.0 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 55, localhost, executor driver, partition 0, PROCESS_LOCAL, 5013 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 19.0 (TID 55)
18/01/26 13:12:32 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131232_0019_m_000000_55' to file:/tmp/unfit-lr-model/stages/0_tok_7d09eee91cc4/metadata/_temporary/0/task_20180126131232_0019_m_000000
18/01/26 13:12:32 INFO SparkHadoopMapRedUtil: attempt_20180126131232_0019_m_000000_55: Committed
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 19.0 (TID 55). 794 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 55) in 10 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 19 (saveAsTextFile at ReadWrite.scala:278) finished in 0.010 s
18/01/26 13:12:32 INFO DAGScheduler: Job 18 finished: saveAsTextFile at ReadWrite.scala:278, took 0.020314 s
18/01/26 13:12:32 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:32 INFO DAGScheduler: Got job 19 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 20 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[41] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 20,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 20,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 41,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 41,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[41] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 40}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 40,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[40] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 19
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 20)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 19:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 23
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 19
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 64.2 KB, free 365.9 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 22.5 KB, free 365.9 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 56, localhost, executor driver, partition 0, PROCESS_LOCAL, 5057 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 20.0 (TID 56)
18/01/26 13:12:32 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131232_0020_m_000000_56' to file:/tmp/unfit-lr-model/stages/1_hashingTF_c46078c6277d/metadata/_temporary/0/task_20180126131232_0020_m_000000
18/01/26 13:12:32 INFO SparkHadoopMapRedUtil: attempt_20180126131232_0020_m_000000_56: Committed
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 20.0 (TID 56). 794 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 56) in 10 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 20 (saveAsTextFile at ReadWrite.scala:278) finished in 0.010 s
18/01/26 13:12:32 INFO DAGScheduler: Job 19 finished: saveAsTextFile at ReadWrite.scala:278, took 0.020291 s
18/01/26 13:12:32 INFO SparkContext: Starting job: saveAsTextFile at ReadWrite.scala:278
18/01/26 13:12:32 INFO DAGScheduler: Got job 20 (saveAsTextFile at ReadWrite.scala:278) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 21 (saveAsTextFile at ReadWrite.scala:278)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[43] at saveAsTextFile at ReadWrite.scala:278), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 21,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 21,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 43,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 43,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[43] at saveAsTextFile at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 42}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 42,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[42] at parallelize at ReadWrite.scala:278,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 20
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 21)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 20:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 24
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 20
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 64.2 KB, free 365.8 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 22.5 KB, free 365.8 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 138.96.200.169:42291 (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at saveAsTextFile at ReadWrite.scala:278) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 57, localhost, executor driver, partition 0, PROCESS_LOCAL, 5294 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 21.0 (TID 57)
18/01/26 13:12:32 INFO FileOutputCommitter: Saved output of task 'attempt_20180126131232_0021_m_000000_57' to file:/tmp/unfit-lr-model/stages/2_logreg_ab3e78168e03/metadata/_temporary/0/task_20180126131232_0021_m_000000
18/01/26 13:12:32 INFO SparkHadoopMapRedUtil: attempt_20180126131232_0021_m_000000_57: Committed
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 21.0 (TID 57). 794 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 57) in 11 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 21 (saveAsTextFile at ReadWrite.scala:278) finished in 0.012 s
18/01/26 13:12:32 INFO DAGScheduler: Job 20 finished: saveAsTextFile at ReadWrite.scala:278, took 0.023683 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 216.6 KB, free 365.6 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 20.5 KB, free 365.6 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 34 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 21 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 22 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 22 (/tmp/spark-logistic-regression-model/metadata MapPartitionsRDD[45] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 22,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 22,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 45,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 45,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/metadata MapPartitionsRDD[45] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 44}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 44,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/metadata HadoopRDD[44] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 21
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 22)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 21:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 25
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 21
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 3.8 KB, free 365.6 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.2 KB, free 365.6 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 138.96.200.169:42291 (size: 2.2 KB, free: 366.2 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (/tmp/spark-logistic-regression-model/metadata MapPartitionsRDD[45] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 58, localhost, executor driver, partition 0, PROCESS_LOCAL, 4880 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 22.0 (TID 58)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/metadata/part-00000:0+227
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 22.0 (TID 58). 1026 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 58) in 18 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 22 (first at ReadWrite.scala:382) finished in 0.018 s
18/01/26 13:12:32 INFO DAGScheduler: Job 21 finished: first at ReadWrite.scala:382, took 0.023005 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 216.7 KB, free 365.4 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 20.5 KB, free 365.3 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 36 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 22 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 23 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 23 (/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata MapPartitionsRDD[47] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 23,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 23,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 47,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 47,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata MapPartitionsRDD[47] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 46}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 46,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata HadoopRDD[46] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 22
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 23)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 22:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 26
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 22
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 3.9 KB, free 365.3 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 2.3 KB, free 365.3 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 138.96.200.169:42291 (size: 2.3 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata MapPartitionsRDD[47] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 59, localhost, executor driver, partition 0, PROCESS_LOCAL, 4906 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 23.0 (TID 59)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata/part-00000:0+184
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 23.0 (TID 59). 980 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 59) in 3 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 23 (first at ReadWrite.scala:382) finished in 0.004 s
18/01/26 13:12:32 INFO DAGScheduler: Job 22 finished: first at ReadWrite.scala:382, took 0.007899 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 216.7 KB, free 365.1 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 20.5 KB, free 365.1 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 38 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 23 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 24 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 24 (/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata MapPartitionsRDD[49] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 24,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 24,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 49,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 49,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata MapPartitionsRDD[49] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 48}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 48,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata HadoopRDD[48] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 23
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 24)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 23:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 27
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 23
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 3.9 KB, free 365.1 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 2.3 KB, free 365.1 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 138.96.200.169:42291 (size: 2.3 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata MapPartitionsRDD[49] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 60, localhost, executor driver, partition 0, PROCESS_LOCAL, 4906 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 24.0 (TID 60)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/stages/0_tok_7d09eee91cc4/metadata/part-00000:0+184
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 24.0 (TID 60). 980 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 60) in 3 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 24 (first at ReadWrite.scala:382) finished in 0.004 s
18/01/26 13:12:32 INFO DAGScheduler: Job 23 finished: first at ReadWrite.scala:382, took 0.007394 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 216.7 KB, free 364.9 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 20.5 KB, free 364.9 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 40 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 24 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 25 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 25 (/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata MapPartitionsRDD[51] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 25,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 25,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 51,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 51,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata MapPartitionsRDD[51] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 50}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 50,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata HadoopRDD[50] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 24
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 25)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 24:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 28
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 24
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 3.9 KB, free 364.9 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.3 KB, free 364.9 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 138.96.200.169:42291 (size: 2.3 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata MapPartitionsRDD[51] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 61, localhost, executor driver, partition 0, PROCESS_LOCAL, 4912 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 25.0 (TID 61)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata/part-00000:0+228
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 25.0 (TID 61). 1027 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 61) in 4 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 25 (first at ReadWrite.scala:382) finished in 0.005 s
18/01/26 13:12:32 INFO DAGScheduler: Job 24 finished: first at ReadWrite.scala:382, took 0.008149 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 216.7 KB, free 364.7 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 20.5 KB, free 364.6 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 42 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 25 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 26 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 26 (/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata MapPartitionsRDD[53] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 26,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 26,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 53,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 53,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata MapPartitionsRDD[53] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 52}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 52,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata HadoopRDD[52] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 25
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 26)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 25:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 29
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 25
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 3.9 KB, free 364.6 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 2.3 KB, free 364.6 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 138.96.200.169:42291 (size: 2.3 KB, free: 366.1 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata MapPartitionsRDD[53] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 62, localhost, executor driver, partition 0, PROCESS_LOCAL, 4912 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 26.0 (TID 62)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/stages/1_hashingTF_c46078c6277d/metadata/part-00000:0+228
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 26.0 (TID 62). 1027 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 62) in 5 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 26 (first at ReadWrite.scala:382) finished in 0.006 s
18/01/26 13:12:32 INFO DAGScheduler: Job 25 finished: first at ReadWrite.scala:382, took 0.010505 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 216.7 KB, free 364.4 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 20.5 KB, free 364.4 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.0 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 44 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 26 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 27 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 27 (/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata MapPartitionsRDD[55] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 27,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 27,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 55,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 55,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata MapPartitionsRDD[55] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 54}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 54,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata HadoopRDD[54] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 26
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 27)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 26:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 30
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 26
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 3.9 KB, free 364.4 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 2.3 KB, free 364.4 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 138.96.200.169:42291 (size: 2.3 KB, free: 366.0 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata MapPartitionsRDD[55] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 63, localhost, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 27.0 (TID 63)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata/part-00000:0+470
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 27.0 (TID 63). 1269 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 63) in 4 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 27 (first at ReadWrite.scala:382) finished in 0.005 s
18/01/26 13:12:32 INFO DAGScheduler: Job 26 finished: first at ReadWrite.scala:382, took 0.009336 s
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 216.7 KB, free 364.2 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 20.5 KB, free 364.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 138.96.200.169:42291 (size: 20.5 KB, free: 366.0 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 46 from textFile at ReadWrite.scala:382
18/01/26 13:12:32 INFO FileInputFormat: Total input paths to process : 1
18/01/26 13:12:32 INFO SparkContext: Starting job: first at ReadWrite.scala:382
18/01/26 13:12:32 INFO DAGScheduler: Got job 27 (first at ReadWrite.scala:382) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 28 (first at ReadWrite.scala:382)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 28 (/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata MapPartitionsRDD[57] at textFile at ReadWrite.scala:382), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 28,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 28,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 57,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 57,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata MapPartitionsRDD[57] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 56}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 56,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: /tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata HadoopRDD[56] at textFile at ReadWrite.scala:382,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 27
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 28)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 27:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 31
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 27
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 3.9 KB, free 364.2 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 2.3 KB, free 364.2 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 138.96.200.169:42291 (size: 2.3 KB, free: 366.0 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata MapPartitionsRDD[57] at textFile at ReadWrite.scala:382) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 4909 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 28.0 (TID 64)
18/01/26 13:12:32 INFO HadoopRDD: Input split: file:/tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/metadata/part-00000:0+470
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 28.0 (TID 64). 1269 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 64) in 5 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 28 (first at ReadWrite.scala:382) finished in 0.005 s
18/01/26 13:12:32 INFO DAGScheduler: Job 27 finished: first at ReadWrite.scala:382, took 0.009598 s
18/01/26 13:12:32 INFO SparkContext: Starting job: load at LogisticRegression.scala:1201
18/01/26 13:12:32 INFO DAGScheduler: Got job 28 (load at LogisticRegression.scala:1201) with 1 output partitions
18/01/26 13:12:32 INFO DAGScheduler: Final stage: ResultStage 29 (load at LogisticRegression.scala:1201)
18/01/26 13:12:32 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:32 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:32 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[59] at load at LogisticRegression.scala:1201), which has no missing parents
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   id: 29,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 29,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 59,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 59,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[59] at load at LogisticRegression.scala:1201,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 58}
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     {id: 58,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      name: ParallelCollectionRDD[58] at load at LogisticRegression.scala:1201,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:32 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Starting for job = 28
18/01/26 13:12:32 WARN Simulator: || SIMULATION || submitStage(ResultStage 29)
18/01/26 13:12:32 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Statistics for 28:
18/01/26 13:12:32 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:32 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:32 WARN Simulator: || SIMULATION || narrowDependencies = 32
18/01/26 13:12:32 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:32 WARN Simulator: || SIMULATION || Finished for job = 28
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 108.2 KB, free 364.0 MB)
18/01/26 13:12:32 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 30.4 KB, free 364.0 MB)
18/01/26 13:12:32 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 138.96.200.169:42291 (size: 30.4 KB, free: 366.0 MB)
18/01/26 13:12:32 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at load at LogisticRegression.scala:1201) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:32 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
18/01/26 13:12:32 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 65, localhost, executor driver, partition 0, PROCESS_LOCAL, 5069 bytes)
18/01/26 13:12:32 INFO Executor: Running task 0.0 in stage 29.0 (TID 65)
18/01/26 13:12:32 INFO Executor: Finished task 0.0 in stage 29.0 (TID 65). 1855 bytes result sent to driver
18/01/26 13:12:32 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 65) in 68 ms on localhost (executor driver) (1/1)
18/01/26 13:12:32 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
18/01/26 13:12:32 INFO DAGScheduler: ResultStage 29 (load at LogisticRegression.scala:1201) finished in 0.068 s
18/01/26 13:12:32 INFO DAGScheduler: Job 28 finished: load at LogisticRegression.scala:1201, took 0.079871 s
18/01/26 13:12:33 INFO FileSourceStrategy: Pruning directories with: 
18/01/26 13:12:33 INFO FileSourceStrategy: Post-Scan Filters: 
18/01/26 13:12:33 INFO FileSourceStrategy: Output Data Schema: struct<numClasses: int, numFeatures: int, interceptVector: vector, coefficientMatrix: matrix, isMultinomial: boolean ... 3 more fields>
18/01/26 13:12:33 INFO FileSourceScanExec: Pushed Filters: 
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 11.266295 ms
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 22.222071 ms
18/01/26 13:12:33 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 318.5 KB, free 363.7 MB)
18/01/26 13:12:33 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 29.9 KB, free 363.7 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 138.96.200.169:42291 (size: 29.9 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO SparkContext: Created broadcast 49 from head at LogisticRegression.scala:1219
18/01/26 13:12:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
18/01/26 13:12:33 INFO SparkContext: Starting job: head at LogisticRegression.scala:1219
18/01/26 13:12:33 INFO DAGScheduler: Got job 29 (head at LogisticRegression.scala:1219) with 1 output partitions
18/01/26 13:12:33 INFO DAGScheduler: Final stage: ResultStage 30 (head at LogisticRegression.scala:1219)
18/01/26 13:12:33 INFO DAGScheduler: Parents of final stage: List()
18/01/26 13:12:33 INFO DAGScheduler: Missing parents: List()
18/01/26 13:12:33 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[62] at head at LogisticRegression.scala:1219), which has no missing parents
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO || {
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||   id: 30,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 30,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 62,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||   rdds: [
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||     {id: 62,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[62] at head at LogisticRegression.scala:1219,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 61}
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||     {id: 61,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[61] at head at LogisticRegression.scala:1219,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 60}
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||     },
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||     {id: 60,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[60] at head at LogisticRegression.scala:1219,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      paths: (1,1),
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      deps: [ 
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||      ]
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||     }
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO ||   ]
18/01/26 13:12:33 WARN DAGScheduler: || DAGINFO || }
18/01/26 13:12:33 WARN Simulator: || SIMULATION || Starting for job = 29
18/01/26 13:12:33 WARN Simulator: || SIMULATION || submitStage(ResultStage 30)
18/01/26 13:12:33 WARN Simulator: || SIMULATION || missing: List()
18/01/26 13:12:33 WARN Simulator: || SIMULATION || Statistics for 29:
18/01/26 13:12:33 WARN Simulator: || SIMULATION || hits = 11
18/01/26 13:12:33 WARN Simulator: || SIMULATION || misses = 1
18/01/26 13:12:33 WARN Simulator: || SIMULATION || diskHits = 0
18/01/26 13:12:33 WARN Simulator: || SIMULATION || narrowDependencies = 34
18/01/26 13:12:33 WARN Simulator: || SIMULATION || shuffleDpendencies = 1
18/01/26 13:12:33 WARN Simulator: || SIMULATION || Finished for job = 29
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 138.96.200.169:42291 in memory (size: 2.3 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO ContextCleaner: Cleaned accumulator 411
18/01/26 13:12:33 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 18.4 KB, free 363.7 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 138.96.200.169:42291 in memory (size: 2.2 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 6.7 KB, free 363.7 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 138.96.200.169:42291 (size: 6.7 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1141
18/01/26 13:12:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[62] at head at LogisticRegression.scala:1219) (first 15 tasks are for partitions Vector(0))
18/01/26 13:12:33 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
18/01/26 13:12:33 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 66, localhost, executor driver, partition 0, PROCESS_LOCAL, 5382 bytes)
18/01/26 13:12:33 INFO ContextCleaner: Cleaned shuffle 0
18/01/26 13:12:33 INFO Executor: Running task 0.0 in stage 30.0 (TID 66)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 138.96.200.169:42291 in memory (size: 2.3 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO ContextCleaner: Cleaned accumulator 412
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.0 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 138.96.200.169:42291 in memory (size: 2.3 KB, free: 366.1 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.1 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 138.96.200.169:42291 in memory (size: 2.3 KB, free: 366.1 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.1 MB)
18/01/26 13:12:33 INFO FileScanRDD: Reading File path: file:///tmp/spark-logistic-regression-model/stages/2_logreg_ab3e78168e03/data/part-00000-ba853735-8111-45b7-8bc7-2b15c25b4bac-c000.snappy.parquet, range: 0-3988, partition values: [empty row]
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 138.96.200.169:42291 in memory (size: 30.4 KB, free: 366.1 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.1 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.2 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 138.96.200.169:42291 in memory (size: 33.4 KB, free: 366.2 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.2 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 138.96.200.169:42291 in memory (size: 2.3 KB, free: 366.2 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 138.96.200.169:42291 in memory (size: 2.3 KB, free: 366.2 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 138.96.200.169:42291 in memory (size: 22.5 KB, free: 366.2 MB)
18/01/26 13:12:33 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 138.96.200.169:42291 in memory (size: 20.5 KB, free: 366.3 MB)
18/01/26 13:12:33 INFO ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  required int32 numClasses;
  required int32 numFeatures;
  optional group interceptVector {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
  optional group coefficientMatrix {
    required int32 type (INT_8);
    required int32 numRows;
    required int32 numCols;
    optional group colPtrs (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group rowIndices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
    required boolean isTransposed;
  }
  required boolean isMultinomial;
}

Catalyst form:
StructType(StructField(numClasses,IntegerType,true), StructField(numFeatures,IntegerType,true), StructField(interceptVector,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true), StructField(coefficientMatrix,org.apache.spark.ml.linalg.MatrixUDT@e59e0c69,true), StructField(isMultinomial,BooleanType,true))
       
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 22.385991 ms
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 11.730119 ms
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 13.812683 ms
18/01/26 13:12:33 INFO InternalParquetRecordReader: RecordReader initialized will read a total of 1 records.
18/01/26 13:12:33 INFO InternalParquetRecordReader: at row 0. reading next block
18/01/26 13:12:33 INFO CodecPool: Got brand-new decompressor [.snappy]
18/01/26 13:12:33 INFO InternalParquetRecordReader: block read in memory in 5 ms. row count = 1
18/01/26 13:12:33 INFO Executor: Finished task 0.0 in stage 30.0 (TID 66). 1400 bytes result sent to driver
18/01/26 13:12:33 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 66) in 143 ms on localhost (executor driver) (1/1)
18/01/26 13:12:33 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
18/01/26 13:12:33 INFO DAGScheduler: ResultStage 30 (head at LogisticRegression.scala:1219) finished in 0.144 s
18/01/26 13:12:33 INFO DAGScheduler: Job 29 finished: head at LogisticRegression.scala:1219, took 0.181761 s
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 12.409827 ms
18/01/26 13:12:33 INFO CodeGenerator: Code generated in 14.713512 ms
18/01/26 13:12:33 INFO AbstractConnector: Stopped Spark@54a3ab8f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18/01/26 13:12:33 INFO SparkUI: Stopped Spark web UI at http://138.96.200.169:4040
18/01/26 13:12:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/01/26 13:12:33 INFO MemoryStore: MemoryStore cleared
18/01/26 13:12:33 INFO BlockManager: BlockManager stopped
18/01/26 13:12:33 INFO BlockManagerMaster: BlockManagerMaster stopped
18/01/26 13:12:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/01/26 13:12:33 INFO SparkContext: Successfully stopped SparkContext
18/01/26 13:12:33 INFO ShutdownHookManager: Shutdown hook called
18/01/26 13:12:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-80bcecf4-8889-4e9b-bae0-5ca94df5b27e

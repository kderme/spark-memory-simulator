Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/12/18 18:27:07 INFO SparkContext: Running Spark version 2.2.2-SNAPSHOT
17/12/18 18:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/18 18:27:07 INFO SparkContext: Submitted application: LogisticRegressionWithElasticNetExample
17/12/18 18:27:07 INFO SecurityManager: Changing view acls to: kderment
17/12/18 18:27:07 INFO SecurityManager: Changing modify acls to: kderment
17/12/18 18:27:07 INFO SecurityManager: Changing view acls groups to: 
17/12/18 18:27:07 INFO SecurityManager: Changing modify acls groups to: 
17/12/18 18:27:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kderment); groups with view permissions: Set(); users  with modify permissions: Set(kderment); groups with modify permissions: Set()
17/12/18 18:27:07 INFO Utils: Successfully started service 'sparkDriver' on port 37789.
17/12/18 18:27:07 INFO SparkEnv: Registering MapOutputTracker
17/12/18 18:27:07 INFO SparkEnv: Registering BlockManagerMaster
17/12/18 18:27:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/18 18:27:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/18 18:27:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1ff50433-d3dd-48e7-aaa1-34f45569a375
17/12/18 18:27:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/12/18 18:27:08 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/18 18:27:08 INFO log: Logging initialized @1348ms
17/12/18 18:27:08 INFO Server: jetty-9.3.11.v20160721
17/12/18 18:27:08 INFO Server: Started @1418ms
17/12/18 18:27:08 INFO AbstractConnector: Started ServerConnector@710d188a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/12/18 18:27:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ef41c66{/jobs,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61e3a1fd{/jobs/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@eadb475{/jobs/job,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3fc08eec{/jobs/job/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b02e036{/stages,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1e287667{/stages/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4201a617{/stages/stage,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@df5f5c0{/stages/stage/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@66b72664{/stages/pool,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@58cd06cb{/stages/pool/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64b31700{/storage,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@bae47a0{/storage/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@85ec632{/storage/rdd,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@65ef722a{/storage/rdd/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@214894fc{/environment,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e362c57{/environment/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79c4715d{/executors,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6548bb7d{/executors/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54336c81{/executors/threadDump,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35e52059{/executors/threadDump/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49bd54f7{/static,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17ae98d7{/,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ac4944a{/api,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cc9ce8{/jobs/job/kill,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c0b41d6{/stages/stage/kill,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://138.96.200.169:4040
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/htrace-core-3.0.4.jar at spark://138.96.200.169:37789/jars/htrace-core-3.0.4.jar with timestamp 1513618028252
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.annotation-api-1.2.jar at spark://138.96.200.169:37789/jars/javax.annotation-api-1.2.jar with timestamp 1513618028252
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028253
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/osgi-resource-locator-1.0.1.jar at spark://138.96.200.169:37789/jars/osgi-resource-locator-1.0.1.jar with timestamp 1513618028253
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-ipc-1.7.7.jar at spark://138.96.200.169:37789/jars/avro-ipc-1.7.7.jar with timestamp 1513618028253
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-library-2.11.8.jar at spark://138.96.200.169:37789/jars/scala-library-2.11.8.jar with timestamp 1513618028253
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028253
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-configuration-1.6.jar at spark://138.96.200.169:37789/jars/commons-configuration-1.6.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/ST4-4.0.4.jar at spark://138.96.200.169:37789/jars/ST4-4.0.4.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xercesImpl-2.9.1.jar at spark://138.96.200.169:37789/jars/xercesImpl-2.9.1.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jcl-over-slf4j-1.7.16.jar at spark://138.96.200.169:37789/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jets3t-0.9.3.jar at spark://138.96.200.169:37789/jars/jets3t-0.9.3.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/breeze-macros_2.11-0.13.2.jar at spark://138.96.200.169:37789/jars/breeze-macros_2.11-0.13.2.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.inject-2.4.0-b34.jar at spark://138.96.200.169:37789/jars/javax.inject-2.4.0-b34.jar with timestamp 1513618028254
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/compress-lzf-1.0.3.jar at spark://138.96.200.169:37789/jars/compress-lzf-1.0.3.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/validation-api-1.1.0.Final.jar at spark://138.96.200.169:37789/jars/validation-api-1.1.0.Final.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-dbcp-1.4.jar at spark://138.96.200.169:37789/jars/commons-dbcp-1.4.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/JavaEWAH-0.3.2.jar at spark://138.96.200.169:37789/jars/JavaEWAH-0.3.2.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-servlet-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-servlet-9.3.11.v20160721.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scopt_2.11-3.3.0.jar at spark://138.96.200.169:37789/jars/scopt_2.11-3.3.0.jar with timestamp 1513618028255
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/chill_2.11-0.8.0.jar at spark://138.96.200.169:37789/jars/chill_2.11-0.8.0.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-container-servlet-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-container-servlet-2.22.2.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/servlet-api-2.5-20110124.jar at spark://138.96.200.169:37789/jars/servlet-api-2.5-20110124.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-client-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-client-9.3.11.v20160721.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/calcite-linq4j-1.2.0-incubating.jar at spark://138.96.200.169:37789/jars/calcite-linq4j-1.2.0-incubating.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-beanutils-core-1.8.0.jar at spark://138.96.200.169:37789/jars/commons-beanutils-core-1.8.0.jar with timestamp 1513618028256
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/java-xmlbuilder-1.0.jar at spark://138.96.200.169:37789/jars/java-xmlbuilder-1.0.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-hdfs-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-hdfs-2.6.5.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stax-api-1.0.1.jar at spark://138.96.200.169:37789/jars/stax-api-1.0.1.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jtransforms-2.4.0.jar at spark://138.96.200.169:37789/jars/jtransforms-2.4.0.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-common-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-common-2.6.5.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-sdk-1.6.0.jar at spark://138.96.200.169:37789/jars/flume-ng-sdk-1.6.0.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/pyrolite-4.13.jar at spark://138.96.200.169:37789/jars/pyrolite-4.13.jar with timestamp 1513618028257
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javassist-3.18.1-GA.jar at spark://138.96.200.169:37789/jars/javassist-3.18.1-GA.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jettison-1.1.jar at spark://138.96.200.169:37789/jars/jettison-1.1.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-io-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-io-9.3.11.v20160721.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/paranamer-2.6.jar at spark://138.96.200.169:37789/jars/paranamer-2.6.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-module-paranamer-2.6.5.jar at spark://138.96.200.169:37789/jars/jackson-module-paranamer-2.6.5.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/ivy-2.4.0.jar at spark://138.96.200.169:37789/jars/ivy-2.4.0.jar with timestamp 1513618028258
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-crypto-1.0.0.jar at spark://138.96.200.169:37789/jars/commons-crypto-1.0.0.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-app-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-app-2.6.5.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/httpclient-4.5.2.jar at spark://138.96.200.169:37789/jars/httpclient-4.5.2.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/curator-recipes-2.6.0.jar at spark://138.96.200.169:37789/jars/curator-recipes-2.6.0.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/libfb303-0.9.3.jar at spark://138.96.200.169:37789/jars/libfb303-0.9.3.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-media-jaxb-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/api-util-1.0.0-M20.jar at spark://138.96.200.169:37789/jars/api-util-1.0.0-M20.jar with timestamp 1513618028259
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-json-3.1.2.jar at spark://138.96.200.169:37789/jars/metrics-json-3.1.2.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hive-exec-1.2.1.spark2.jar at spark://138.96.200.169:37789/jars/hive-exec-1.2.1.spark2.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-util-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-util-9.3.11.v20160721.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spire_2.11-0.13.0.jar at spark://138.96.200.169:37789/jars/spire_2.11-0.13.0.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hk2-locator-2.4.0-b34.jar at spark://138.96.200.169:37789/jars/hk2-locator-2.4.0-b34.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/velocity-1.7.jar at spark://138.96.200.169:37789/jars/velocity-1.7.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-core-1.6.0.jar at spark://138.96.200.169:37789/jars/flume-ng-core-1.6.0.jar with timestamp 1513618028260
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-io-2.4.jar at spark://138.96.200.169:37789/jars/commons-io-2.4.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-container-servlet-core-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/apache-log4j-extras-1.2.17.jar at spark://138.96.200.169:37789/jars/apache-log4j-extras-1.2.17.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-reflect-2.11.8.jar at spark://138.96.200.169:37789/jars/scala-reflect-2.11.8.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.servlet-api-3.1.0.jar at spark://138.96.200.169:37789/jars/javax.servlet-api-3.1.0.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-encoding-1.8.2.jar at spark://138.96.200.169:37789/jars/parquet-encoding-1.8.2.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-server-common-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-yarn-server-common-2.6.5.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-xc-1.9.13.jar at spark://138.96.200.169:37789/jars/jackson-xc-1.9.13.jar with timestamp 1513618028261
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-pool-1.5.4.jar at spark://138.96.200.169:37789/jars/commons-pool-1.5.4.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-databind-2.6.5.jar at spark://138.96.200.169:37789/jars/jackson-databind-2.6.5.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/mina-core-2.0.4.jar at spark://138.96.200.169:37789/jars/mina-core-2.0.4.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-api-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-yarn-api-2.6.5.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/gson-2.2.4.jar at spark://138.96.200.169:37789/jars/gson-2.2.4.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/datanucleus-api-jdo-3.2.6.jar at spark://138.96.200.169:37789/jars/datanucleus-api-jdo-3.2.6.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-compiler-3.0.7.jar at spark://138.96.200.169:37789/jars/commons-compiler-3.0.7.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-common-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-yarn-common-2.6.5.jar with timestamp 1513618028262
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-core-asl-1.9.13.jar at spark://138.96.200.169:37789/jars/jackson-core-asl-1.9.13.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-collections-3.2.2.jar at spark://138.96.200.169:37789/jars/commons-collections-3.2.2.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-logging-1.2.jar at spark://138.96.200.169:37789/jars/commons-logging-1.2.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-common-1.8.2.jar at spark://138.96.200.169:37789/jars/parquet-common-1.8.2.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/unused-1.0.0.jar at spark://138.96.200.169:37789/jars/unused-1.0.0.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-digester-1.8.jar at spark://138.96.200.169:37789/jars/commons-digester-1.8.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/bcprov-jdk15on-1.51.jar at spark://138.96.200.169:37789/jars/bcprov-jdk15on-1.51.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stream-2.7.0.jar at spark://138.96.200.169:37789/jars/stream-2.7.0.jar with timestamp 1513618028263
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jdo-api-3.0.1.jar at spark://138.96.200.169:37789/jars/jdo-api-3.0.1.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/guava-14.0.1.jar at spark://138.96.200.169:37789/jars/guava-14.0.1.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-1.7.7.jar at spark://138.96.200.169:37789/jars/avro-1.7.7.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hk2-utils-2.4.0-b34.jar at spark://138.96.200.169:37789/jars/hk2-utils-2.4.0-b34.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-server-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-server-9.3.11.v20160721.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.inject-1.jar at spark://138.96.200.169:37789/jars/javax.inject-1.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xmlenc-0.52.jar at spark://138.96.200.169:37789/jars/xmlenc-0.52.jar with timestamp 1513618028264
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-hadoop-bundle-1.6.0.jar at spark://138.96.200.169:37789/jars/parquet-hadoop-bundle-1.6.0.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/shapeless_2.11-2.3.2.jar at spark://138.96.200.169:37789/jars/shapeless_2.11-2.3.2.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-6.1.26.jar at spark://138.96.200.169:37789/jars/jetty-6.1.26.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-plus-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-plus-9.3.11.v20160721.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-lang3-3.5.jar at spark://138.96.200.169:37789/jars/commons-lang3-3.5.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/aopalliance-1.0.jar at spark://138.96.200.169:37789/jars/aopalliance-1.0.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jaxb-api-2.2.2.jar at spark://138.96.200.169:37789/jars/jaxb-api-2.2.2.jar with timestamp 1513618028265
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/curator-client-2.6.0.jar at spark://138.96.200.169:37789/jars/curator-client-2.6.0.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-core-3.1.2.jar at spark://138.96.200.169:37789/jars/metrics-core-3.1.2.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-cli-1.2.jar at spark://138.96.200.169:37789/jars/commons-cli-1.2.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spire-macros_2.11-0.13.0.jar at spark://138.96.200.169:37789/jars/spire-macros_2.11-0.13.0.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-jvm-3.1.2.jar at spark://138.96.200.169:37789/jars/metrics-jvm-3.1.2.jar with timestamp 1513618028266
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-column-1.8.2.jar at spark://138.96.200.169:37789/jars/parquet-column-1.8.2.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/janino-3.0.7.jar at spark://138.96.200.169:37789/jars/janino-3.0.7.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/guice-3.0.jar at spark://138.96.200.169:37789/jars/guice-3.0.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-ipc-1.7.7-tests.jar at spark://138.96.200.169:37789/jars/avro-ipc-1.7.7-tests.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/mail-1.4.7.jar at spark://138.96.200.169:37789/jars/mail-1.4.7.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-client-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-client-2.22.2.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/pmml-schema-1.2.15.jar at spark://138.96.200.169:37789/jars/pmml-schema-1.2.15.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/py4j-0.10.4.jar at spark://138.96.200.169:37789/jars/py4j-0.10.4.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-auth-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-auth-2.6.5.jar with timestamp 1513618028267
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/bonecp-0.8.0.RELEASE.jar at spark://138.96.200.169:37789/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-client-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-client-2.6.5.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-mapper-asl-1.9.13.jar at spark://138.96.200.169:37789/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/antlr-runtime-3.4.jar at spark://138.96.200.169:37789/jars/antlr-runtime-3.4.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-guava-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-guava-2.22.2.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-annotations-2.6.5.jar at spark://138.96.200.169:37789/jars/jackson-annotations-2.6.5.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scalap-2.11.8.jar at spark://138.96.200.169:37789/jars/scalap-2.11.8.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-common-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-common-2.22.2.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-mapred-1.7.7-hadoop2.jar at spark://138.96.200.169:37789/jars/avro-mapred-1.7.7-hadoop2.jar with timestamp 1513618028268
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/RoaringBitmap-0.5.11.jar at spark://138.96.200.169:37789/jars/RoaringBitmap-0.5.11.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/objenesis-2.1.jar at spark://138.96.200.169:37789/jars/objenesis-2.1.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stax-api-1.0-2.jar at spark://138.96.200.169:37789/jars/stax-api-1.0-2.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/univocity-parsers-2.2.1.jar at spark://138.96.200.169:37789/jars/univocity-parsers-2.2.1.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-graphite-3.1.2.jar at spark://138.96.200.169:37789/jars/metrics-graphite-3.1.2.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/activation-1.1.1.jar at spark://138.96.200.169:37789/jars/activation-1.1.1.jar with timestamp 1513618028269
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/mx4j-3.0.2.jar at spark://138.96.200.169:37789/jars/mx4j-3.0.2.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/minlog-1.3.0.jar at spark://138.96.200.169:37789/jars/minlog-1.3.0.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/protobuf-java-2.5.0.jar at spark://138.96.200.169:37789/jars/protobuf-java-2.5.0.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/pmml-model-1.2.15.jar at spark://138.96.200.169:37789/jars/pmml-model-1.2.15.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/base64-2.3.8.jar at spark://138.96.200.169:37789/jars/base64-2.3.8.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/metrics-core-2.2.0.jar at spark://138.96.200.169:37789/jars/metrics-core-2.2.0.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/antlr-2.7.7.jar at spark://138.96.200.169:37789/jars/antlr-2.7.7.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/chill-java-0.8.0.jar at spark://138.96.200.169:37789/jars/chill-java-0.8.0.jar with timestamp 1513618028270
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/netty-all-4.0.43.Final.jar at spark://138.96.200.169:37789/jars/netty-all-4.0.43.Final.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/slf4j-api-1.7.21.jar at spark://138.96.200.169:37789/jars/slf4j-api-1.7.21.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/cglib-2.2.1-v20090111.jar at spark://138.96.200.169:37789/jars/cglib-2.2.1-v20090111.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-http-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-http-9.3.11.v20160721.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/slf4j-log4j12-1.7.16.jar at spark://138.96.200.169:37789/jars/slf4j-log4j12-1.7.16.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-hadoop-1.8.2.jar at spark://138.96.200.169:37789/jars/parquet-hadoop-1.8.2.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hk2-api-2.4.0-b34.jar at spark://138.96.200.169:37789/jars/hk2-api-2.4.0-b34.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/core-1.1.2.jar at spark://138.96.200.169:37789/jars/core-1.1.2.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/api-asn1-api-1.0.0-M20.jar at spark://138.96.200.169:37789/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1513618028271
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/json4s-jackson_2.11-3.2.11.jar at spark://138.96.200.169:37789/jars/json4s-jackson_2.11-3.2.11.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-net-3.1.jar at spark://138.96.200.169:37789/jars/commons-net-3.1.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/avro-compiler-1.7.3.jar at spark://138.96.200.169:37789/jars/avro-compiler-1.7.3.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-webapp-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-webapp-9.3.11.v20160721.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-jndi-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-jndi-9.3.11.v20160721.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jsr305-1.3.9.jar at spark://138.96.200.169:37789/jars/jsr305-1.3.9.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-auth-1.6.0.jar at spark://138.96.200.169:37789/jars/flume-ng-auth-1.6.0.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-lang-2.6.jar at spark://138.96.200.169:37789/jars/commons-lang-2.6.jar with timestamp 1513618028272
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/apacheds-i18n-2.0.0-M15.jar at spark://138.96.200.169:37789/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-continuation-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-continuation-9.3.11.v20160721.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-format-2.3.1.jar at spark://138.96.200.169:37789/jars/parquet-format-2.3.1.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/datanucleus-rdbms-3.2.9.jar at spark://138.96.200.169:37789/jars/datanucleus-rdbms-3.2.9.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jul-to-slf4j-1.7.16.jar at spark://138.96.200.169:37789/jars/jul-to-slf4j-1.7.16.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/datanucleus-core-3.2.10.jar at spark://138.96.200.169:37789/jars/datanucleus-core-3.2.10.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hive-metastore-1.2.1.spark2.jar at spark://138.96.200.169:37789/jars/hive-metastore-1.2.1.spark2.jar with timestamp 1513618028273
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-annotations-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-annotations-2.6.5.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jodd-core-3.5.2.jar at spark://138.96.200.169:37789/jars/jodd-core-3.5.2.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/json4s-ast_2.11-3.2.11.jar at spark://138.96.200.169:37789/jars/json4s-ast_2.11-3.2.11.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-util-6.1.26.jar at spark://138.96.200.169:37789/jars/jetty-util-6.1.26.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-compress-1.4.1.jar at spark://138.96.200.169:37789/jars/commons-compress-1.4.1.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/zkclient-0.3.jar at spark://138.96.200.169:37789/jars/zkclient-0.3.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/macro-compat_2.11-1.1.1.jar at spark://138.96.200.169:37789/jars/macro-compat_2.11-1.1.1.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-xml_2.11-1.0.4.jar at spark://138.96.200.169:37789/jars/scala-xml_2.11-1.0.4.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-parser-combinators_2.11-1.0.4.jar at spark://138.96.200.169:37789/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/eigenbase-properties-1.1.5.jar at spark://138.96.200.169:37789/jars/eigenbase-properties-1.1.5.jar with timestamp 1513618028274
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xz-1.0.jar at spark://138.96.200.169:37789/jars/xz-1.0.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javax.ws.rs-api-2.0.1.jar at spark://138.96.200.169:37789/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/zookeeper-3.4.6.jar at spark://138.96.200.169:37789/jars/zookeeper-3.4.6.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/json4s-core_2.11-3.2.11.jar at spark://138.96.200.169:37789/jars/json4s-core_2.11-3.2.11.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-module-scala_2.11-2.6.5.jar at spark://138.96.200.169:37789/jars/jackson-module-scala_2.11-2.6.5.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/antlr4-runtime-4.5.3.jar at spark://138.96.200.169:37789/jars/antlr4-runtime-4.5.3.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-yarn-client-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-yarn-client-2.6.5.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/oro-2.0.8.jar at spark://138.96.200.169:37789/jars/oro-2.0.8.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/libthrift-0.9.3.jar at spark://138.96.200.169:37789/jars/libthrift-0.9.3.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-proxy-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-proxy-9.3.11.v20160721.jar with timestamp 1513618028275
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-beanutils-1.7.0.jar at spark://138.96.200.169:37789/jars/commons-beanutils-1.7.0.jar with timestamp 1513618028276
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/joda-time-2.9.3.jar at spark://138.96.200.169:37789/jars/joda-time-2.9.3.jar with timestamp 1513618028276
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xbean-asm5-shaded-4.4.jar at spark://138.96.200.169:37789/jars/xbean-asm5-shaded-4.4.jar with timestamp 1513618028276
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/derby-10.12.1.1.jar at spark://138.96.200.169:37789/jars/derby-10.12.1.1.jar with timestamp 1513618028276
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jersey-server-2.22.2.jar at spark://138.96.200.169:37789/jars/jersey-server-2.22.2.jar with timestamp 1513618028276
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/kryo-shaded-3.0.3.jar at spark://138.96.200.169:37789/jars/kryo-shaded-3.0.3.jar with timestamp 1513618028276
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-math3-3.4.1.jar at spark://138.96.200.169:37789/jars/commons-math3-3.4.1.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/calcite-core-1.2.0-incubating.jar at spark://138.96.200.169:37789/jars/calcite-core-1.2.0-incubating.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-xml-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-xml-9.3.11.v20160721.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-httpclient-3.1.jar at spark://138.96.200.169:37789/jars/commons-httpclient-3.1.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/opencsv-2.3.jar at spark://138.96.200.169:37789/jars/opencsv-2.3.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/xml-apis-1.3.04.jar at spark://138.96.200.169:37789/jars/xml-apis-1.3.04.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-servlets-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-servlets-9.3.11.v20160721.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jetty-security-9.3.11.v20160721.jar at spark://138.96.200.169:37789/jars/jetty-security-9.3.11.v20160721.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-core-2.6.5.jar at spark://138.96.200.169:37789/jars/jackson-core-2.6.5.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jline-0.9.94.jar at spark://138.96.200.169:37789/jars/jline-0.9.94.jar with timestamp 1513618028277
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/lz4-1.3.0.jar at spark://138.96.200.169:37789/jars/lz4-1.3.0.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/aopalliance-repackaged-2.4.0-b34.jar at spark://138.96.200.169:37789/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/arpack_combined_all-0.1.jar at spark://138.96.200.169:37789/jars/arpack_combined_all-0.1.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/snappy-0.2.jar at spark://138.96.200.169:37789/jars/snappy-0.2.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/httpcore-4.4.4.jar at spark://138.96.200.169:37789/jars/httpcore-4.4.4.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/commons-codec-1.10.jar at spark://138.96.200.169:37789/jars/commons-codec-1.10.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jackson-jaxrs-1.9.13.jar at spark://138.96.200.169:37789/jars/jackson-jaxrs-1.9.13.jar with timestamp 1513618028278
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/snappy-java-1.1.2.6.jar at spark://138.96.200.169:37789/jars/snappy-java-1.1.2.6.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/breeze_2.11-0.13.2.jar at spark://138.96.200.169:37789/jars/breeze_2.11-0.13.2.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/flume-ng-configuration-1.6.0.jar at spark://138.96.200.169:37789/jars/flume-ng-configuration-1.6.0.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/javolution-5.5.1.jar at spark://138.96.200.169:37789/jars/javolution-5.5.1.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/kafka_2.11-0.8.2.1.jar at spark://138.96.200.169:37789/jars/kafka_2.11-0.8.2.1.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/netty-3.9.9.Final.jar at spark://138.96.200.169:37789/jars/netty-3.9.9.Final.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/machinist_2.11-0.6.1.jar at spark://138.96.200.169:37789/jars/machinist_2.11-0.6.1.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/scala-compiler-2.11.8.jar at spark://138.96.200.169:37789/jars/scala-compiler-2.11.8.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/parquet-jackson-1.8.2.jar at spark://138.96.200.169:37789/jars/parquet-jackson-1.8.2.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/leveldbjni-all-1.8.jar at spark://138.96.200.169:37789/jars/leveldbjni-all-1.8.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/stringtemplate-3.2.1.jar at spark://138.96.200.169:37789/jars/stringtemplate-3.2.1.jar with timestamp 1513618028279
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/apacheds-kerberos-codec-2.0.0-M15.jar at spark://138.96.200.169:37789/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/calcite-avatica-1.2.0-incubating.jar at spark://138.96.200.169:37789/jars/calcite-avatica-1.2.0-incubating.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar at spark://138.96.200.169:37789/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-common-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-common-2.6.5.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/curator-framework-2.6.0.jar at spark://138.96.200.169:37789/jars/curator-framework-2.6.0.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/jta-1.1.jar at spark://138.96.200.169:37789/jars/jta-1.1.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/kafka-clients-0.10.0.1.jar at spark://138.96.200.169:37789/jars/kafka-clients-0.10.0.1.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/hadoop-mapreduce-client-core-2.6.5.jar at spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-core-2.6.5.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO SparkContext: Added JAR file:/workspaces/kderment/repos/spark/examples/target/scala-2.11/jars/log4j-1.2.17.jar at spark://138.96.200.169:37789/jars/log4j-1.2.17.jar with timestamp 1513618028280
17/12/18 18:27:08 INFO Executor: Starting executor ID driver on host localhost
17/12/18 18:27:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44343.
17/12/18 18:27:08 INFO NettyBlockTransferService: Server created on 138.96.200.169:44343
17/12/18 18:27:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/18 18:27:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 138.96.200.169, 44343, None)
17/12/18 18:27:08 INFO BlockManagerMasterEndpoint: Registering block manager 138.96.200.169:44343 with 366.3 MB RAM, BlockManagerId(driver, 138.96.200.169, 44343, None)
17/12/18 18:27:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 138.96.200.169, 44343, None)
17/12/18 18:27:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 138.96.200.169, 44343, None)
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6a969fb8{/metrics/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/local/workspaces/kderment/repos/spark/spark-warehouse/').
17/12/18 18:27:08 INFO SharedState: Warehouse path is 'file:/local/workspaces/kderment/repos/spark/spark-warehouse/'.
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@388b401d{/SQL,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77a281fc{/SQL/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a7761b1{/SQL/execution,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@27fde870{/SQL/execution/json,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@293cde83{/static/sql,null,AVAILABLE,@Spark}
17/12/18 18:27:08 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
17/12/18 18:27:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 216.6 KB, free 366.1 MB)
17/12/18 18:27:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 366.1 MB)
17/12/18 18:27:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 138.96.200.169:44343 (size: 20.5 KB, free: 366.3 MB)
17/12/18 18:27:09 INFO SparkContext: Created broadcast 0 from textFile at MLUtils.scala:99
17/12/18 18:27:09 INFO FileInputFormat: Total input paths to process : 1
17/12/18 18:27:09 INFO SparkContext: Starting job: reduce at MLUtils.scala:92
17/12/18 18:27:09 INFO DAGScheduler: Got job 0 (reduce at MLUtils.scala:92) with 8 output partitions
17/12/18 18:27:09 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at MLUtils.scala:92)
17/12/18 18:27:09 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:09 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at map at MLUtils.scala:90), which has no missing parents
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||   id: 0,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 0,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 5,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     {id: 5,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[5] at map at MLUtils.scala:90,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      partitions_number: 8,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 4}
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     {id: 4,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[4] at map at MLUtils.scala:102,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      partitions_number: 8,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 3}
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     {id: 3,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[3] at filter at MLUtils.scala:101,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      partitions_number: 8,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 2}
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     {id: 2,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[2] at map at MLUtils.scala:100,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      partitions_number: 8,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 1}
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     {id: 1,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      name: file:///local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt MapPartitionsRDD[1] at textFile at MLUtils.scala:99,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      partitions_number: 8,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 0}
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     {id: 0,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      name: file:///local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt HadoopRDD[0] at textFile at MLUtils.scala:99,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      partitions_number: 8,
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:09 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KB, free 366.1 MB)
17/12/18 18:27:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 366.1 MB)
17/12/18 18:27:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 138.96.200.169:44343 (size: 2.3 KB, free: 366.3 MB)
17/12/18 18:27:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:09 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at map at MLUtils.scala:90) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/18 18:27:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/12/18 18:27:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 4896 bytes)
17/12/18 18:27:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/12/18 18:27:09 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/12/18 18:27:09 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
17/12/18 18:27:09 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
17/12/18 18:27:09 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)
17/12/18 18:27:09 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)
17/12/18 18:27:09 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)
17/12/18 18:27:09 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scala-xml_2.11-1.0.4.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO TransportClientFactory: Successfully created connection to /138.96.200.169:37789 after 25 ms (0 ms spent in bootstraps)
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scala-xml_2.11-1.0.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3154713015790497414.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scala-xml_2.11-1.0.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-jndi-9.3.11.v20160721.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-jndi-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp613171811978414083.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-jndi-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/ivy-2.4.0.jar with timestamp 1513618028258
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/ivy-2.4.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1078938825164023525.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/ivy-2.4.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javax.servlet-api-3.1.0.jar with timestamp 1513618028261
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javax.servlet-api-3.1.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3729212281254075420.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javax.servlet-api-3.1.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/protobuf-java-2.5.0.jar with timestamp 1513618028270
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/protobuf-java-2.5.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8994855848699999730.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/protobuf-java-2.5.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/slf4j-api-1.7.21.jar with timestamp 1513618028271
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/slf4j-api-1.7.21.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp443246480808428928.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/slf4j-api-1.7.21.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-security-9.3.11.v20160721.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-security-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2549570789712789423.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-security-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/apacheds-kerberos-codec-2.0.0-M15.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/apacheds-kerberos-codec-2.0.0-M15.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp9079043662025176755.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/apacheds-kerberos-codec-2.0.0-M15.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jtransforms-2.4.0.jar with timestamp 1513618028257
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jtransforms-2.4.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1651199992917533582.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jtransforms-2.4.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-webapp-9.3.11.v20160721.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-webapp-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2843362971944249056.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-webapp-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-client-2.6.5.jar with timestamp 1513618028268
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-client-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3981549712278710782.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-client-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-digester-1.8.jar with timestamp 1513618028263
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-digester-1.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3244309473119046535.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-digester-1.8.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/core-1.1.2.jar with timestamp 1513618028271
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/core-1.1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4109631492984400591.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/core-1.1.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028260
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-network-common_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp9124640490831240308.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-network-common_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-dbcp-1.4.jar with timestamp 1513618028255
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-dbcp-1.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3511957331211738676.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-dbcp-1.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-jackson-1.8.2.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-jackson-1.8.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5506678485852128238.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-jackson-1.8.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/httpclient-4.5.2.jar with timestamp 1513618028259
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/httpclient-4.5.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8170884099288238777.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/httpclient-4.5.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-encoding-1.8.2.jar with timestamp 1513618028261
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-encoding-1.8.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2805592824748813467.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-encoding-1.8.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hive-exec-1.2.1.spark2.jar with timestamp 1513618028260
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hive-exec-1.2.1.spark2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp508732166048529766.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hive-exec-1.2.1.spark2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/chill-java-0.8.0.jar with timestamp 1513618028270
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/chill-java-0.8.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6519507883759256600.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/chill-java-0.8.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-hive_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4585679638759329966.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-hive_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/avro-compiler-1.7.3.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/avro-compiler-1.7.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1165434414352224897.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/avro-compiler-1.7.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/activation-1.1.1.jar with timestamp 1513618028269
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/activation-1.1.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7222363688455016617.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/activation-1.1.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jul-to-slf4j-1.7.16.jar with timestamp 1513618028273
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jul-to-slf4j-1.7.16.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6815782145893913667.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jul-to-slf4j-1.7.16.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/log4j-1.2.17.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/log4j-1.2.17.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp858740650707164941.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/log4j-1.2.17.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/flume-ng-sdk-1.6.0.jar with timestamp 1513618028257
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/flume-ng-sdk-1.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8443137541905963789.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/flume-ng-sdk-1.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-module-scala_2.11-2.6.5.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-module-scala_2.11-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4103296753823198181.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-module-scala_2.11-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/datanucleus-api-jdo-3.2.6.jar with timestamp 1513618028262
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/datanucleus-api-jdo-3.2.6.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4476709983798034946.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/datanucleus-api-jdo-3.2.6.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hk2-api-2.4.0-b34.jar with timestamp 1513618028271
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hk2-api-2.4.0-b34.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp450100785539872445.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hk2-api-2.4.0-b34.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-server-9.3.11.v20160721.jar with timestamp 1513618028264
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-server-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5783440558115918455.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-server-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/avro-ipc-1.7.7.jar with timestamp 1513618028253
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/avro-ipc-1.7.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4018121363224384387.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/avro-ipc-1.7.7.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-lang3-3.5.jar with timestamp 1513618028265
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-lang3-3.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8274992141667825350.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-lang3-3.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/zookeeper-3.4.6.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/zookeeper-3.4.6.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5333304945597577291.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/zookeeper-3.4.6.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-server-2.22.2.jar with timestamp 1513618028276
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-server-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp52398173976449527.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-server-2.22.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/api-asn1-api-1.0.0-M20.jar with timestamp 1513618028271
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/api-asn1-api-1.0.0-M20.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1528231062328555054.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/api-asn1-api-1.0.0-M20.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-6.1.26.jar with timestamp 1513618028265
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-6.1.26.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp442971738955626203.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-6.1.26.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jets3t-0.9.3.jar with timestamp 1513618028254
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jets3t-0.9.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp764483801858780365.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jets3t-0.9.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-pool-1.5.4.jar with timestamp 1513618028262
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-pool-1.5.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6790461419565010027.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-pool-1.5.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/pyrolite-4.13.jar with timestamp 1513618028257
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/pyrolite-4.13.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7502618544067153292.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/pyrolite-4.13.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-cli-1.2.jar with timestamp 1513618028266
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-cli-1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp718760802517540386.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-cli-1.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028278
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5152913320749660542.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-network-shuffle_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/slf4j-log4j12-1.7.16.jar with timestamp 1513618028271
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/slf4j-log4j12-1.7.16.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2720495297692080524.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/slf4j-log4j12-1.7.16.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-guava-2.22.2.jar with timestamp 1513618028268
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-guava-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5865079944335149507.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-guava-2.22.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/json4s-ast_2.11-3.2.11.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/json4s-ast_2.11-3.2.11.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6580716459414254454.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/json4s-ast_2.11-3.2.11.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-io-9.3.11.v20160721.jar with timestamp 1513618028258
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-io-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7795339849590113384.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-io-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028266
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7974172621609081244.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-catalyst_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/netty-3.9.9.Final.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/netty-3.9.9.Final.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3482859926273673254.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/netty-3.9.9.Final.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/arpack_combined_all-0.1.jar with timestamp 1513618028278
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/arpack_combined_all-0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1596614702768034290.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/arpack_combined_all-0.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/datanucleus-core-3.2.10.jar with timestamp 1513618028273
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/datanucleus-core-3.2.10.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp742328685337102027.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/datanucleus-core-3.2.10.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/chill_2.11-0.8.0.jar with timestamp 1513618028256
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/chill_2.11-0.8.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1583821031651192101.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/chill_2.11-0.8.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-plus-9.3.11.v20160721.jar with timestamp 1513618028265
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-plus-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7110687039439300403.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-plus-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/joda-time-2.9.3.jar with timestamp 1513618028276
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/joda-time-2.9.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5015541391670833290.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/joda-time-2.9.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/antlr4-runtime-4.5.3.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/antlr4-runtime-4.5.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8156464206080259376.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/antlr4-runtime-4.5.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scala-compiler-2.11.8.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scala-compiler-2.11.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2327182642068627170.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scala-compiler-2.11.8.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/stringtemplate-3.2.1.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/stringtemplate-3.2.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp465756940305940157.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/stringtemplate-3.2.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/xmlenc-0.52.jar with timestamp 1513618028264
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/xmlenc-0.52.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8688453034892868476.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/xmlenc-0.52.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/aopalliance-repackaged-2.4.0-b34.jar with timestamp 1513618028278
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/aopalliance-repackaged-2.4.0-b34.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2731638532437812590.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/aopalliance-repackaged-2.4.0-b34.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-servlets-9.3.11.v20160721.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-servlets-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2648693265522108407.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-servlets-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/lz4-1.3.0.jar with timestamp 1513618028278
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/lz4-1.3.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2310609424825895995.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/lz4-1.3.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/mail-1.4.7.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/mail-1.4.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3119177107968552189.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/mail-1.4.7.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/calcite-core-1.2.0-incubating.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/calcite-core-1.2.0-incubating.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1161819770505368276.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/calcite-core-1.2.0-incubating.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/bonecp-0.8.0.RELEASE.jar with timestamp 1513618028268
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/bonecp-0.8.0.RELEASE.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6039934554563163146.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/bonecp-0.8.0.RELEASE.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jettison-1.1.jar with timestamp 1513618028258
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jettison-1.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6321887869119264680.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jettison-1.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/py4j-0.10.4.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/py4j-0.10.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8893730780705788995.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/py4j-0.10.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/oro-2.0.8.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/oro-2.0.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp9218083286514915625.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/oro-2.0.8.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/curator-recipes-2.6.0.jar with timestamp 1513618028259
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/curator-recipes-2.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2590214799275768874.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/curator-recipes-2.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/ST4-4.0.4.jar with timestamp 1513618028254
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/ST4-4.0.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3097959958078583894.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/ST4-4.0.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-util-6.1.26.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-util-6.1.26.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7224043824090274327.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-util-6.1.26.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/netty-all-4.0.43.Final.jar with timestamp 1513618028271
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/netty-all-4.0.43.Final.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3682250843288102285.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/netty-all-4.0.43.Final.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/objenesis-2.1.jar with timestamp 1513618028269
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/objenesis-2.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4583461827763630859.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/objenesis-2.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-client-2.6.5.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-client-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4812966631618404118.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-yarn-client-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-format-2.3.1.jar with timestamp 1513618028273
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-format-2.3.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2797004005896643729.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-format-2.3.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-proxy-9.3.11.v20160721.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-proxy-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8245064784403870049.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-proxy-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/metrics-json-3.1.2.jar with timestamp 1513618028260
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/metrics-json-3.1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7407063837031397737.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/metrics-json-3.1.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028269
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-sketch_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3060296821282133939.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-sketch_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/guava-14.0.1.jar with timestamp 1513618028264
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/guava-14.0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2494713576182841804.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/guava-14.0.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5148524249930276006.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-sql-kafka-0-10_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-configuration-1.6.jar with timestamp 1513618028254
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-configuration-1.6.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp248308786227801751.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-configuration-1.6.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/bcprov-jdk15on-1.51.jar with timestamp 1513618028263
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/bcprov-jdk15on-1.51.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3801933029722970650.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/bcprov-jdk15on-1.51.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/httpcore-4.4.4.jar with timestamp 1513618028278
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/httpcore-4.4.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1844498354472866508.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/httpcore-4.4.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/eigenbase-properties-1.1.5.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/eigenbase-properties-1.1.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3444281019635226347.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/eigenbase-properties-1.1.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hk2-utils-2.4.0-b34.jar with timestamp 1513618028264
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hk2-utils-2.4.0-b34.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2063732359328960673.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hk2-utils-2.4.0-b34.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-httpclient-3.1.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-httpclient-3.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3926201674411889729.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-httpclient-3.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-container-servlet-core-2.22.2.jar with timestamp 1513618028261
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-container-servlet-core-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1743343220254147389.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-container-servlet-core-2.22.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-net-3.1.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-net-3.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2894179498804695448.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-net-3.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-compress-1.4.1.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-compress-1.4.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3373001698361773653.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-compress-1.4.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/avro-ipc-1.7.7-tests.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/avro-ipc-1.7.7-tests.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1282999776922090278.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/avro-ipc-1.7.7-tests.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-hadoop-bundle-1.6.0.jar with timestamp 1513618028265
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-hadoop-bundle-1.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp150983545526284538.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-hadoop-bundle-1.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/xz-1.0.jar with timestamp 1513618028275
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/xz-1.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4374263288993396497.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/xz-1.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-module-paranamer-2.6.5.jar with timestamp 1513618028258
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-module-paranamer-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2651061713773219511.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-module-paranamer-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spire-macros_2.11-0.13.0.jar with timestamp 1513618028266
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spire-macros_2.11-0.13.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3412678222606660698.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spire-macros_2.11-0.13.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/gson-2.2.4.jar with timestamp 1513618028262
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/gson-2.2.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp161152419349815633.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/gson-2.2.4.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028273
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7445155186333236702.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-streaming-kafka-0-8_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-common-2.6.5.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-common-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp361876963788334308.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-mapreduce-client-common-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jaxb-api-2.2.2.jar with timestamp 1513618028265
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jaxb-api-2.2.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5364388098041728366.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jaxb-api-2.2.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-client-2.22.2.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-client-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7653852284706019644.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-client-2.22.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/metrics-core-3.1.2.jar with timestamp 1513618028266
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/metrics-core-3.1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3196876642750012152.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/metrics-core-3.1.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javax.inject-2.4.0-b34.jar with timestamp 1513618028254
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javax.inject-2.4.0-b34.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5023106552376954197.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javax.inject-2.4.0-b34.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028256
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1206088115639463912.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-streaming-flume_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-core-2.6.5.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-core-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1107005442087571117.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-mapreduce-client-core-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jsr305-1.3.9.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jsr305-1.3.9.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp907247672170804291.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jsr305-1.3.9.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/zkclient-0.3.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/zkclient-0.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6968813061218598347.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/zkclient-0.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/velocity-1.7.jar with timestamp 1513618028260
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/velocity-1.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4578580288040242273.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/velocity-1.7.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jta-1.1.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jta-1.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2337530908930008770.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jta-1.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/univocity-parsers-2.2.1.jar with timestamp 1513618028269
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/univocity-parsers-2.2.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2249351384561712553.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/univocity-parsers-2.2.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/paranamer-2.6.jar with timestamp 1513618028258
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/paranamer-2.6.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8506282296142198563.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/paranamer-2.6.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/flume-ng-auth-1.6.0.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/flume-ng-auth-1.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1417959615398129959.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/flume-ng-auth-1.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/mx4j-3.0.2.jar with timestamp 1513618028270
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/mx4j-3.0.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4795638302840857724.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/mx4j-3.0.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jodd-core-3.5.2.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jodd-core-3.5.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp421944563186611034.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jodd-core-3.5.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/derby-10.12.1.1.jar with timestamp 1513618028276
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/derby-10.12.1.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp9002890307449150745.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/derby-10.12.1.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javolution-5.5.1.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javolution-5.5.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2968509983740634674.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javolution-5.5.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scala-library-2.11.8.jar with timestamp 1513618028253
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scala-library-2.11.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp966962162519171123.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scala-library-2.11.8.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-common-2.6.5.jar with timestamp 1513618028257
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-common-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4287225606982454592.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-common-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/libfb303-0.9.3.jar with timestamp 1513618028259
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/libfb303-0.9.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp9067825142697879387.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/libfb303-0.9.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-auth-2.6.5.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-auth-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5837248729594237341.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-auth-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-lang-2.6.jar with timestamp 1513618028272
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-lang-2.6.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3514761773058306878.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-lang-2.6.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/flume-ng-configuration-1.6.0.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/flume-ng-configuration-1.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1666863688196185600.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/flume-ng-configuration-1.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/base64-2.3.8.jar with timestamp 1513618028270
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/base64-2.3.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2809220401261909433.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/base64-2.3.8.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/macro-compat_2.11-1.1.1.jar with timestamp 1513618028274
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/macro-compat_2.11-1.1.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4245705790168766598.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/macro-compat_2.11-1.1.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jcl-over-slf4j-1.7.16.jar with timestamp 1513618028254
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jcl-over-slf4j-1.7.16.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7401497915892686610.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jcl-over-slf4j-1.7.16.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-servlet-9.3.11.v20160721.jar with timestamp 1513618028255
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-servlet-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3436900368286027553.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-servlet-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hive-metastore-1.2.1.spark2.jar with timestamp 1513618028273
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hive-metastore-1.2.1.spark2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3236738032649976454.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hive-metastore-1.2.1.spark2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/shapeless_2.11-2.3.2.jar with timestamp 1513618028265
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/shapeless_2.11-2.3.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7326073547382137714.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/shapeless_2.11-2.3.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/stax-api-1.0.1.jar with timestamp 1513618028257
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/stax-api-1.0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp19786535833718890.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/stax-api-1.0.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-beanutils-1.7.0.jar with timestamp 1513618028276
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-beanutils-1.7.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5299727871304797279.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-beanutils-1.7.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/kafka-clients-0.10.0.1.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/kafka-clients-0.10.0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8024452704281229350.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/kafka-clients-0.10.0.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jline-0.9.94.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jline-0.9.94.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp896822157279163954.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jline-0.9.94.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/guice-3.0.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/guice-3.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5087497948505651465.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/guice-3.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-column-1.8.2.jar with timestamp 1513618028267
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-column-1.8.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3756102580297730507.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-column-1.8.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/RoaringBitmap-0.5.11.jar with timestamp 1513618028269
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/RoaringBitmap-0.5.11.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4321762443486697980.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/RoaringBitmap-0.5.11.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028256
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-launcher_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8307264691364585070.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-launcher_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/breeze-macros_2.11-0.13.2.jar with timestamp 1513618028254
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/breeze-macros_2.11-0.13.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4351671586522979.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/breeze-macros_2.11-0.13.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/validation-api-1.1.0.Final.jar with timestamp 1513618028255
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/validation-api-1.1.0.Final.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6203256656274746984.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/validation-api-1.1.0.Final.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/curator-client-2.6.0.jar with timestamp 1513618028266
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/curator-client-2.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1157060684031436261.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/curator-client-2.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/compress-lzf-1.0.3.jar with timestamp 1513618028255
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/compress-lzf-1.0.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2248040340542099748.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/compress-lzf-1.0.3.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/curator-framework-2.6.0.jar with timestamp 1513618028280
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/curator-framework-2.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5610745264974681572.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/curator-framework-2.6.0.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-api-2.6.5.jar with timestamp 1513618028262
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-api-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6611245522043880357.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-yarn-api-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-server-common-2.6.5.jar with timestamp 1513618028261
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-server-common-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6832894022058895436.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-yarn-server-common-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-core-2.6.5.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-core-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp9051171558490627445.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-core-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/machinist_2.11-0.6.1.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/machinist_2.11-0.6.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8343331082216158775.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/machinist_2.11-0.6.1.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar with timestamp 1513618028277
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1722604062870057089.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-mapreduce-client-shuffle-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar with timestamp 1513618028266
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3844930754539830100.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-mapreduce-client-jobclient-2.6.5.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-util-9.3.11.v20160721.jar with timestamp 1513618028260
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-util-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7831324485252999483.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-util-9.3.11.v20160721.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-collections-3.2.2.jar with timestamp 1513618028263
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-collections-3.2.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8820366172630039962.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-collections-3.2.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-container-servlet-2.22.2.jar with timestamp 1513618028256
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-container-servlet-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5410146459496687752.tmp
17/12/18 18:27:09 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-container-servlet-2.22.2.jar to class loader
17/12/18 18:27:09 INFO Executor: Fetching spark://138.96.200.169:37789/jars/breeze_2.11-0.13.2.jar with timestamp 1513618028279
17/12/18 18:27:09 INFO Utils: Fetching spark://138.96.200.169:37789/jars/breeze_2.11-0.13.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6474387240292610311.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/breeze_2.11-0.13.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-compiler-3.0.7.jar with timestamp 1513618028262
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-compiler-3.0.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7049169878067974433.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-compiler-3.0.7.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/pmml-schema-1.2.15.jar with timestamp 1513618028267
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/pmml-schema-1.2.15.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3699216491422732029.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/pmml-schema-1.2.15.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/kryo-shaded-3.0.3.jar with timestamp 1513618028276
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/kryo-shaded-3.0.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4989195570926411340.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/kryo-shaded-3.0.3.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/antlr-runtime-3.4.jar with timestamp 1513618028268
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/antlr-runtime-3.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2176919956703715861.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/antlr-runtime-3.4.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/apache-log4j-extras-1.2.17.jar with timestamp 1513618028261
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/apache-log4j-extras-1.2.17.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8380126057924569522.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/apache-log4j-extras-1.2.17.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/apacheds-i18n-2.0.0-M15.jar with timestamp 1513618028273
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/apacheds-i18n-2.0.0-M15.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp180007803189480219.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/apacheds-i18n-2.0.0-M15.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/json4s-jackson_2.11-3.2.11.jar with timestamp 1513618028272
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/json4s-jackson_2.11-3.2.11.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2137376246341918663.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/json4s-jackson_2.11-3.2.11.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/metrics-graphite-3.1.2.jar with timestamp 1513618028269
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/metrics-graphite-3.1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8809263242664783214.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/metrics-graphite-3.1.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/leveldbjni-all-1.8.jar with timestamp 1513618028279
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/leveldbjni-all-1.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp352377321796939120.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/leveldbjni-all-1.8.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/snappy-java-1.1.2.6.jar with timestamp 1513618028279
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/snappy-java-1.1.2.6.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1243628959398242418.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/snappy-java-1.1.2.6.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/janino-3.0.7.jar with timestamp 1513618028267
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/janino-3.0.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp98736687571652346.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/janino-3.0.7.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-media-jaxb-2.22.2.jar with timestamp 1513618028259
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-media-jaxb-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5404799863696617124.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-media-jaxb-2.22.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/calcite-linq4j-1.2.0-incubating.jar with timestamp 1513618028256
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/calcite-linq4j-1.2.0-incubating.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5918845370309806689.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/calcite-linq4j-1.2.0-incubating.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/stax-api-1.0-2.jar with timestamp 1513618028269
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/stax-api-1.0-2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6663112940478983655.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/stax-api-1.0-2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jdo-api-3.0.1.jar with timestamp 1513618028264
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jdo-api-3.0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2008659056040159449.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jdo-api-3.0.1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-xml-9.3.11.v20160721.jar with timestamp 1513618028277
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-xml-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8073390456787196818.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-xml-9.3.11.v20160721.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/stream-2.7.0.jar with timestamp 1513618028263
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/stream-2.7.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1689407623021857775.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/stream-2.7.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028255
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-mllib_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp954721792951540665.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-mllib_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-core-asl-1.9.13.jar with timestamp 1513618028263
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-core-asl-1.9.13.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2755591358699588246.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-core-asl-1.9.13.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028270
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-sql_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp631061461572335956.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-sql_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/JavaEWAH-0.3.2.jar with timestamp 1513618028255
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/JavaEWAH-0.3.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2541092830976835778.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/JavaEWAH-0.3.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javax.annotation-api-1.2.jar with timestamp 1513618028252
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javax.annotation-api-1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1670240911747370545.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javax.annotation-api-1.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/json4s-core_2.11-3.2.11.jar with timestamp 1513618028275
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/json4s-core_2.11-3.2.11.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4482853622693050469.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/json4s-core_2.11-3.2.11.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/servlet-api-2.5-20110124.jar with timestamp 1513618028256
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/servlet-api-2.5-20110124.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1865785071836665539.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/servlet-api-2.5-20110124.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/libthrift-0.9.3.jar with timestamp 1513618028275
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/libthrift-0.9.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7081003989058690523.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/libthrift-0.9.3.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-logging-1.2.jar with timestamp 1513618028263
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-logging-1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5092712944726002557.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-logging-1.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-io-2.4.jar with timestamp 1513618028261
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-io-2.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1250286101335858980.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-io-2.4.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-jaxrs-1.9.13.jar with timestamp 1513618028278
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-jaxrs-1.9.13.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3608511753603045543.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-jaxrs-1.9.13.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/metrics-jvm-3.1.2.jar with timestamp 1513618028266
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/metrics-jvm-3.1.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7214122696910984688.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/metrics-jvm-3.1.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-codec-1.10.jar with timestamp 1513618028278
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-codec-1.10.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp343321470610029312.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-codec-1.10.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-common-1.8.2.jar with timestamp 1513618028263
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-common-1.8.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7155852044097722623.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-common-1.8.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-app-2.6.5.jar with timestamp 1513618028259
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-mapreduce-client-app-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5131129281122281717.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-mapreduce-client-app-2.6.5.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/opencsv-2.3.jar with timestamp 1513618028277
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/opencsv-2.3.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp496540700549407246.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/opencsv-2.3.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028273
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7251905637486438232.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-streaming-flume-sink_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-beanutils-core-1.8.0.jar with timestamp 1513618028256
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-beanutils-core-1.8.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8988070181975481928.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-beanutils-core-1.8.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/minlog-1.3.0.jar with timestamp 1513618028270
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/minlog-1.3.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8270946822828211622.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/minlog-1.3.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/xml-apis-1.3.04.jar with timestamp 1513618028277
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/xml-apis-1.3.04.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp269027480499659669.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/xml-apis-1.3.04.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/calcite-avatica-1.2.0-incubating.jar with timestamp 1513618028280
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/calcite-avatica-1.2.0-incubating.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp90032807735866369.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/calcite-avatica-1.2.0-incubating.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-annotations-2.6.5.jar with timestamp 1513618028274
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-annotations-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7387237360845169664.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-annotations-2.6.5.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-mapper-asl-1.9.13.jar with timestamp 1513618028268
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-mapper-asl-1.9.13.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5459584207298382864.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-mapper-asl-1.9.13.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/api-util-1.0.0-M20.jar with timestamp 1513618028259
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/api-util-1.0.0-M20.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7881631861893538994.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/api-util-1.0.0-M20.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jersey-common-2.22.2.jar with timestamp 1513618028268
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jersey-common-2.22.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8276057947928510905.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jersey-common-2.22.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-annotations-2.6.5.jar with timestamp 1513618028268
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-annotations-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2210387710964816926.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-annotations-2.6.5.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/xbean-asm5-shaded-4.4.jar with timestamp 1513618028276
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/xbean-asm5-shaded-4.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5471346921454622197.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/xbean-asm5-shaded-4.4.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-client-9.3.11.v20160721.jar with timestamp 1513618028256
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-client-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3637926239425500263.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-client-9.3.11.v20160721.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028258
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-graphx_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6829765001531064487.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-graphx_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-databind-2.6.5.jar with timestamp 1513618028262
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-databind-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3543537470886052892.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-databind-2.6.5.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-math3-3.4.1.jar with timestamp 1513618028277
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-math3-3.4.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1314573099117072242.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-math3-3.4.1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/mina-core-2.0.4.jar with timestamp 1513618028262
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/mina-core-2.0.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3388719681681694064.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/mina-core-2.0.4.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-continuation-9.3.11.v20160721.jar with timestamp 1513618028273
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-continuation-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2726709648524648663.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-continuation-9.3.11.v20160721.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jackson-xc-1.9.13.jar with timestamp 1513618028261
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jackson-xc-1.9.13.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7452799590932573995.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jackson-xc-1.9.13.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028273
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8282707513448670085.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-mllib-local_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-hdfs-2.6.5.jar with timestamp 1513618028257
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-hdfs-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4190638131507236656.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-hdfs-2.6.5.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/unused-1.0.0.jar with timestamp 1513618028263
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/unused-1.0.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3131055431355172456.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/unused-1.0.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scala-reflect-2.11.8.jar with timestamp 1513618028261
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scala-reflect-2.11.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1354160200366863369.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scala-reflect-2.11.8.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/pmml-model-1.2.15.jar with timestamp 1513618028270
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/pmml-model-1.2.15.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7597429518227817423.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/pmml-model-1.2.15.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/avro-1.7.7.jar with timestamp 1513618028264
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/avro-1.7.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3235656634687406283.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/avro-1.7.7.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/parquet-hadoop-1.8.2.jar with timestamp 1513618028271
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/parquet-hadoop-1.8.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7280095056066072638.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/parquet-hadoop-1.8.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/metrics-core-2.2.0.jar with timestamp 1513618028270
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/metrics-core-2.2.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2621170059584469185.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/metrics-core-2.2.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spire_2.11-0.13.0.jar with timestamp 1513618028260
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spire_2.11-0.13.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp685225146294054177.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spire_2.11-0.13.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/aopalliance-1.0.jar with timestamp 1513618028265
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/aopalliance-1.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7330402975213729210.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/aopalliance-1.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hk2-locator-2.4.0-b34.jar with timestamp 1513618028260
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hk2-locator-2.4.0-b34.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2416342461479377797.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hk2-locator-2.4.0-b34.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/htrace-core-3.0.4.jar with timestamp 1513618028252
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/htrace-core-3.0.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1379787123506835374.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/htrace-core-3.0.4.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/datanucleus-rdbms-3.2.9.jar with timestamp 1513618028273
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/datanucleus-rdbms-3.2.9.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp402697934714114517.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/datanucleus-rdbms-3.2.9.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/snappy-0.2.jar with timestamp 1513618028278
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/snappy-0.2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp3521571890806360198.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/snappy-0.2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/commons-crypto-1.0.0.jar with timestamp 1513618028259
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/commons-crypto-1.0.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6978632048686850068.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/commons-crypto-1.0.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scopt_2.11-3.3.0.jar with timestamp 1513618028255
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scopt_2.11-3.3.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4213180282674940665.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scopt_2.11-3.3.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javax.ws.rs-api-2.0.1.jar with timestamp 1513618028275
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javax.ws.rs-api-2.0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4002885176681478174.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javax.ws.rs-api-2.0.1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028264
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-tags_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8806638775821301724.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-tags_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028253
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-streaming_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp121455438869387484.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-streaming_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scalap-2.11.8.jar with timestamp 1513618028268
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scalap-2.11.8.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4911345685395823554.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scalap-2.11.8.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028253
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7974297409779191584.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-unsafe_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028266
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-core_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7327008079305746840.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-core_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-common-2.6.5.jar with timestamp 1513618028262
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/hadoop-yarn-common-2.6.5.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2014110275484953369.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/hadoop-yarn-common-2.6.5.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/jetty-http-9.3.11.v20160721.jar with timestamp 1513618028271
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/jetty-http-9.3.11.v20160721.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7538511597899027440.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/jetty-http-9.3.11.v20160721.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/cglib-2.2.1-v20090111.jar with timestamp 1513618028271
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/cglib-2.2.1-v20090111.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4877801581135468897.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/cglib-2.2.1-v20090111.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javassist-3.18.1-GA.jar with timestamp 1513618028258
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javassist-3.18.1-GA.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1021210064511709138.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javassist-3.18.1-GA.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/osgi-resource-locator-1.0.1.jar with timestamp 1513618028253
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/osgi-resource-locator-1.0.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp6899683761841543330.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/osgi-resource-locator-1.0.1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/avro-mapred-1.7.7-hadoop2.jar with timestamp 1513618028268
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/avro-mapred-1.7.7-hadoop2.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp2888117368682527840.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/avro-mapred-1.7.7-hadoop2.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar with timestamp 1513618028278
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/spark-examples_2.11-2.2.2-SNAPSHOT.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4733766670533419686.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/spark-examples_2.11-2.2.2-SNAPSHOT.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/javax.inject-1.jar with timestamp 1513618028264
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/javax.inject-1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4962335488387082515.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/javax.inject-1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/kafka_2.11-0.8.2.1.jar with timestamp 1513618028279
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/kafka_2.11-0.8.2.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp5993200623003404598.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/kafka_2.11-0.8.2.1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/scala-parser-combinators_2.11-1.0.4.jar with timestamp 1513618028274
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/scala-parser-combinators_2.11-1.0.4.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1400810723650973811.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/scala-parser-combinators_2.11-1.0.4.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/antlr-2.7.7.jar with timestamp 1513618028270
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/antlr-2.7.7.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp8750687771366522502.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/antlr-2.7.7.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/java-xmlbuilder-1.0.jar with timestamp 1513618028257
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/java-xmlbuilder-1.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp1134413436930288133.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/java-xmlbuilder-1.0.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/xercesImpl-2.9.1.jar with timestamp 1513618028254
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/xercesImpl-2.9.1.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp7742619423536687381.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/xercesImpl-2.9.1.jar to class loader
17/12/18 18:27:10 INFO Executor: Fetching spark://138.96.200.169:37789/jars/flume-ng-core-1.6.0.jar with timestamp 1513618028260
17/12/18 18:27:10 INFO Utils: Fetching spark://138.96.200.169:37789/jars/flume-ng-core-1.6.0.jar to /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/fetchFileTemp4220946203070403585.tmp
17/12/18 18:27:10 INFO Executor: Adding file:/tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c/userFiles-e6058c72-92d5-4337-bcfb-5f9dfdf37540/flume-ng-core-1.6.0.jar to class loader
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:91644+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:0+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:39276+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:13092+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:26184+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:78552+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:65460+13092
17/12/18 18:27:10 INFO HadoopRDD: Input split: file:/local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt:52368+13092
17/12/18 18:27:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 953 bytes result sent to driver
17/12/18 18:27:10 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 953 bytes result sent to driver
17/12/18 18:27:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 875 ms on localhost (executor driver) (1/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 866 ms on localhost (executor driver) (2/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 867 ms on localhost (executor driver) (3/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 867 ms on localhost (executor driver) (4/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 868 ms on localhost (executor driver) (5/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 867 ms on localhost (executor driver) (6/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 867 ms on localhost (executor driver) (7/8)
17/12/18 18:27:10 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 867 ms on localhost (executor driver) (8/8)
17/12/18 18:27:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/12/18 18:27:10 INFO DAGScheduler: ResultStage 0 (reduce at MLUtils.scala:92) finished in 0.898 s
17/12/18 18:27:10 INFO DAGScheduler: Job 0 finished: reduce at MLUtils.scala:92, took 1.000017 s
17/12/18 18:27:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 138.96.200.169:44343 in memory (size: 20.5 KB, free: 366.3 MB)
17/12/18 18:27:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 138.96.200.169:44343 in memory (size: 2.3 KB, free: 366.3 MB)
17/12/18 18:27:11 INFO FileSourceStrategy: Pruning directories with: 
17/12/18 18:27:11 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/18 18:27:11 INFO FileSourceStrategy: Output Data Schema: struct<label: double, features: vector>
17/12/18 18:27:11 INFO FileSourceScanExec: Pushed Filters: 
17/12/18 18:27:11 INFO CodeGenerator: Code generated in 182.049837 ms
17/12/18 18:27:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 308.5 KB, free 366.0 MB)
17/12/18 18:27:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 28.3 KB, free 366.0 MB)
17/12/18 18:27:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 138.96.200.169:44343 (size: 28.3 KB, free: 366.3 MB)
17/12/18 18:27:11 INFO SparkContext: Created broadcast 2 from broadcast at LibSVMRelation.scala:153
17/12/18 18:27:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/18 18:27:12 INFO Instrumentation: LogisticRegression-logreg_e01e11021ce6-861500069-1: training: numPartitions=1 storageLevel=StorageLevel(disk, memory, deserialized, 1 replicas)
17/12/18 18:27:12 INFO Instrumentation: LogisticRegression-logreg_e01e11021ce6-861500069-1: {"regParam":0.3,"elasticNetParam":0.8,"maxIter":10}
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:517
17/12/18 18:27:12 INFO DAGScheduler: Got job 1 (treeAggregate at LogisticRegression.scala:517) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 1 (treeAggregate at LogisticRegression.scala:517)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at treeAggregate at LogisticRegression.scala:517), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 11,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 11,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[11] at treeAggregate at LogisticRegression.scala:517,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.4 KB, free 366.0 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.9 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.3 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at treeAggregate at LogisticRegression.scala:517) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 8)
17/12/18 18:27:12 INFO CodeGenerator: Code generated in 13.764569 ms
17/12/18 18:27:12 INFO FileScanRDD: Reading File path: file:///local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]
17/12/18 18:27:12 INFO CodeGenerator: Code generated in 23.235358 ms
17/12/18 18:27:12 INFO CodeGenerator: Code generated in 44.22884 ms
17/12/18 18:27:12 INFO MemoryStore: Block rdd_10_0 stored as values in memory (estimated size 167.5 KB, free 365.8 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added rdd_10_0 in memory on 138.96.200.169:44343 (size: 167.5 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 8). 47614 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 227 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 1 (treeAggregate at LogisticRegression.scala:517) finished in 0.227 s
17/12/18 18:27:12 INFO DAGScheduler: Job 1 finished: treeAggregate at LogisticRegression.scala:517, took 0.241326 s
17/12/18 18:27:12 INFO Instrumentation: LogisticRegression-logreg_e01e11021ce6-861500069-1: {"numClasses":2}
17/12/18 18:27:12 INFO Instrumentation: LogisticRegression-logreg_e01e11021ce6-861500069-1: {"numFeatures":692}
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.4 KB, free 365.8 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.4 KB, free 365.8 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 138.96.200.169:44343 (size: 3.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 4 from broadcast at LogisticRegression.scala:600
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.5 KB, free 365.8 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 175.0 B, free 365.8 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 138.96.200.169:44343 (size: 175.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 5 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 2 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 2 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 2,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 2,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 12,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 12,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[12] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 18.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 9)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 9). 8497 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 9) in 11 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 2 (treeAggregate at LogisticRegression.scala:1894) finished in 0.012 s
17/12/18 18:27:12 INFO DAGScheduler: Job 2 finished: treeAggregate at LogisticRegression.scala:1894, took 0.023017 s
17/12/18 18:27:12 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
17/12/18 18:27:12 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(5) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 138.96.200.169:44343 in memory (size: 175.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 1780.0 B, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 138.96.200.169:44343 (size: 1780.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 7 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 3 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 3 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 3,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 3,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 13,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 13,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[13] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.5 KB, free 365.8 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.8 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 10)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 10). 8497 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 10) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 3 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:12 INFO DAGScheduler: Job 3 finished: treeAggregate at LogisticRegression.scala:1894, took 0.013376 s
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(7) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 138.96.200.169:44343 in memory (size: 1780.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 5.5 KB, free 365.8 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1787.0 B, free 365.8 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 138.96.200.169:44343 (size: 1787.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 9 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 4 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 4 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 4,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 4,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 14,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 14,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[14] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 18.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 4.0 (TID 11)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 4.0 (TID 11). 8540 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 7 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 4 (treeAggregate at LogisticRegression.scala:1894) finished in 0.008 s
17/12/18 18:27:12 INFO DAGScheduler: Job 4 finished: treeAggregate at LogisticRegression.scala:1894, took 0.015903 s
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(9) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 138.96.200.169:44343 in memory (size: 1787.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO OWLQN: Step Size: 0.01032
17/12/18 18:27:12 INFO OWLQN: Val and Grad Norm: 0.666288 (rel: 0.0249) 1.10780
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 5.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 1587.0 B, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 138.96.200.169:44343 (size: 1587.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 11 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 5 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 5 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[15] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 5,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 5,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 15,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 15,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[15] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 18.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[15] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 5.0 (TID 12)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 5.0 (TID 12). 8497 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 12) in 7 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 5 (treeAggregate at LogisticRegression.scala:1894) finished in 0.005 s
17/12/18 18:27:12 INFO DAGScheduler: Job 5 finished: treeAggregate at LogisticRegression.scala:1894, took 0.014928 s
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(11) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 138.96.200.169:44343 in memory (size: 1587.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:12 INFO OWLQN: Val and Grad Norm: 0.621707 (rel: 0.0669) 0.714252
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1324.0 B, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 138.96.200.169:44343 (size: 1324.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 13 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 6 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 6 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[16] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 16,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 16,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[16] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 18.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 6.0 (TID 13)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 6.0 (TID 13). 8497 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 13) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 6 (treeAggregate at LogisticRegression.scala:1894) finished in 0.007 s
17/12/18 18:27:12 INFO DAGScheduler: Job 6 finished: treeAggregate at LogisticRegression.scala:1894, took 0.014197 s
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(13) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 138.96.200.169:44343 in memory (size: 1324.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:12 INFO OWLQN: Val and Grad Norm: 0.612727 (rel: 0.0144) 0.349101
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1069.0 B, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 138.96.200.169:44343 (size: 1069.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 15 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 7 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 7 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[17] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 17,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 17,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[17] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 18.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[17] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 7.0 (TID 14)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 7.0 (TID 14). 8497 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 9 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 7 (treeAggregate at LogisticRegression.scala:1894) finished in 0.009 s
17/12/18 18:27:12 INFO DAGScheduler: Job 7 finished: treeAggregate at LogisticRegression.scala:1894, took 0.017582 s
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(15) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 138.96.200.169:44343 in memory (size: 1069.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:12 INFO OWLQN: Val and Grad Norm: 0.606035 (rel: 0.0109) 0.264884
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.5 KB, free 365.7 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 959.0 B, free 365.7 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 138.96.200.169:44343 (size: 959.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 17 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 8 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 8 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[18] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 18,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 18,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[18] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 18.5 KB, free 365.6 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.6 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 8.0 (TID 15)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 8.0 (TID 15). 8497 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 9 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 8 (treeAggregate at LogisticRegression.scala:1894) finished in 0.010 s
17/12/18 18:27:12 INFO DAGScheduler: Job 8 finished: treeAggregate at LogisticRegression.scala:1894, took 0.017623 s
17/12/18 18:27:12 INFO TorrentBroadcast: Destroying Broadcast(17) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:12 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 138.96.200.169:44343 in memory (size: 959.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:12 INFO OWLQN: Val and Grad Norm: 0.603175 (rel: 0.00472) 0.212559
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 5.5 KB, free 365.6 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 893.0 B, free 365.6 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 138.96.200.169:44343 (size: 893.0 B, free: 366.1 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 19 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:12 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:12 INFO DAGScheduler: Got job 9 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:12 INFO DAGScheduler: Final stage: ResultStage 9 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:12 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:12 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:12 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[19] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 19,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 19,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[19] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:12 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 18.5 KB, free 365.6 MB)
17/12/18 18:27:12 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.6 MB)
17/12/18 18:27:12 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:12 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
17/12/18 18:27:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:12 INFO Executor: Running task 0.0 in stage 9.0 (TID 16)
17/12/18 18:27:12 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:12 INFO Executor: Finished task 0.0 in stage 9.0 (TID 16). 8454 bytes result sent to driver
17/12/18 18:27:12 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 16) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/12/18 18:27:12 INFO DAGScheduler: ResultStage 9 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:12 INFO DAGScheduler: Job 9 finished: treeAggregate at LogisticRegression.scala:1894, took 0.012621 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(19) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 138.96.200.169:44343 in memory (size: 893.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.596962 (rel: 0.0103) 0.219945
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 5.5 KB, free 365.6 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 778.0 B, free 365.6 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 138.96.200.169:44343 (size: 778.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 21 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 10 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 10 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[20] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 20,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 20,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[20] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 18.5 KB, free 365.6 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.6 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[20] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 10.0 (TID 17)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 10.0 (TID 17). 8497 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 17) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 10 (treeAggregate at LogisticRegression.scala:1894) finished in 0.004 s
17/12/18 18:27:13 INFO DAGScheduler: Job 10 finished: treeAggregate at LogisticRegression.scala:1894, took 0.014575 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(21) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 138.96.200.169:44343 in memory (size: 778.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.594074 (rel: 0.00484) 0.192246
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 5.5 KB, free 365.6 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 612.0 B, free 365.6 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 138.96.200.169:44343 (size: 612.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 23 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 11 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 11 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 11,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 11,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 21,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 21,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[21] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 18.5 KB, free 365.6 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.6 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 11.0 (TID 18)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 11.0 (TID 18). 8497 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 18) in 10 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 11 (treeAggregate at LogisticRegression.scala:1894) finished in 0.010 s
17/12/18 18:27:13 INFO DAGScheduler: Job 11 finished: treeAggregate at LogisticRegression.scala:1894, took 0.018734 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(23) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 138.96.200.169:44343 in memory (size: 612.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.590609 (rel: 0.00583) 0.0895147
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 5.5 KB, free 365.6 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 522.0 B, free 365.6 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 138.96.200.169:44343 (size: 522.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 25 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 12 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 12 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[22] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 12,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 12,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 22,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 22,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[22] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[22] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 12.0 (TID 19)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 12.0 (TID 19). 8497 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 19) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 12 (treeAggregate at LogisticRegression.scala:1894) finished in 0.004 s
17/12/18 18:27:13 INFO DAGScheduler: Job 12 finished: treeAggregate at LogisticRegression.scala:1894, took 0.010476 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(25) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 550.0 B, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 138.96.200.169:44343 in memory (size: 522.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 138.96.200.169:44343 (size: 550.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 27 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 13 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 13 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[23] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 13,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 13,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 23,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 23,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[23] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 18.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[23] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 13.0 (TID 20)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 13.0 (TID 20). 8540 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 20) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 13 (treeAggregate at LogisticRegression.scala:1894) finished in 0.008 s
17/12/18 18:27:13 INFO DAGScheduler: Job 13 finished: treeAggregate at LogisticRegression.scala:1894, took 0.015457 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(27) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 138.96.200.169:44343 in memory (size: 550.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 0.5000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.589472 (rel: 0.00192) 0.140102
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 5.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 549.0 B, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 138.96.200.169:44343 (size: 549.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 29 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 14 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 14 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[24] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 14,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 14,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 24,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 24,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[24] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 18.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[24] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 14.0 (TID 21)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 14.0 (TID 21). 8540 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 21) in 7 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 14 (treeAggregate at LogisticRegression.scala:1894) finished in 0.007 s
17/12/18 18:27:13 INFO DAGScheduler: Job 14 finished: treeAggregate at LogisticRegression.scala:1894, took 0.015693 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(29) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 138.96.200.169:44343 in memory (size: 549.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 545.0 B, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 138.96.200.169:44343 (size: 545.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 31 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 15 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 15 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[25] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 15,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 15,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 25,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 25,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[25] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 10}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 10,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[10] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 9}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 9,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[9] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 8}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 8,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[8] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 7}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 7,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[7] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 6}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 6,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[6] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 18.5 KB, free 365.5 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.5 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[25] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 15.0 (TID 22)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_10_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 15.0 (TID 22). 8497 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 22) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 15 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 15 finished: treeAggregate at LogisticRegression.scala:1894, took 0.014700 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(31) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 138.96.200.169:44343 in memory (size: 545.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 0.5000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.588219 (rel: 0.00213) 0.0541654
17/12/18 18:27:13 INFO OWLQN: Converged because max iterations reached
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(4) (from destroy at LogisticRegression.scala:796)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 138.96.200.169:44343 in memory (size: 3.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO MapPartitionsRDD: Removing RDD 10 from persistence list
17/12/18 18:27:13 INFO BlockManager: Removing RDD 10
17/12/18 18:27:13 INFO FileSourceStrategy: Pruning directories with: 
17/12/18 18:27:13 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/18 18:27:13 INFO FileSourceStrategy: Output Data Schema: struct<label: double, features: vector>
17/12/18 18:27:13 INFO FileSourceScanExec: Pushed Filters: 
17/12/18 18:27:13 INFO CodeGenerator: Code generated in 30.4473 ms
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 308.5 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 28.3 KB, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 138.96.200.169:44343 (size: 28.3 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 33 from broadcast at LibSVMRelation.scala:153
17/12/18 18:27:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/18 18:27:13 INFO Instrumentation: LogisticRegression-logreg_e01e11021ce6-861500069-1: training finished
17/12/18 18:27:13 INFO FileSourceStrategy: Pruning directories with: 
17/12/18 18:27:13 INFO FileSourceStrategy: Post-Scan Filters: 
17/12/18 18:27:13 INFO FileSourceStrategy: Output Data Schema: struct<label: double, features: vector>
17/12/18 18:27:13 INFO FileSourceScanExec: Pushed Filters: 
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 308.5 KB, free 365.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 28.3 KB, free 365.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 138.96.200.169:44343 (size: 28.3 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 34 from broadcast at LibSVMRelation.scala:153
17/12/18 18:27:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17/12/18 18:27:13 INFO Instrumentation: LogisticRegression-logreg_268094a70860-451832195-2: training: numPartitions=1 storageLevel=StorageLevel(disk, memory, deserialized, 1 replicas)
17/12/18 18:27:13 INFO Instrumentation: LogisticRegression-logreg_268094a70860-451832195-2: {"regParam":0.3,"elasticNetParam":0.8,"maxIter":10}
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:517
17/12/18 18:27:13 INFO DAGScheduler: Got job 16 (treeAggregate at LogisticRegression.scala:517) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 16 (treeAggregate at LogisticRegression.scala:517)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[36] at treeAggregate at LogisticRegression.scala:517), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 16,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 16,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 36,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 36,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[36] at treeAggregate at LogisticRegression.scala:517,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 18.4 KB, free 365.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 8.3 KB, free 364.9 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 138.96.200.169:44343 (size: 8.3 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[36] at treeAggregate at LogisticRegression.scala:517) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 16.0 (TID 23)
17/12/18 18:27:13 INFO FileScanRDD: Reading File path: file:///local/workspaces/kderment/repos/spark/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]
17/12/18 18:27:13 INFO MemoryStore: Block rdd_35_0 stored as values in memory (estimated size 167.5 KB, free 364.8 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added rdd_35_0 in memory on 138.96.200.169:44343 (size: 167.5 KB, free: 365.9 MB)
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 16.0 (TID 23). 47571 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 23) in 34 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 16 (treeAggregate at LogisticRegression.scala:517) finished in 0.035 s
17/12/18 18:27:13 INFO DAGScheduler: Job 16 finished: treeAggregate at LogisticRegression.scala:517, took 0.044756 s
17/12/18 18:27:13 INFO Instrumentation: LogisticRegression-logreg_268094a70860-451832195-2: {"numClasses":2}
17/12/18 18:27:13 INFO Instrumentation: LogisticRegression-logreg_268094a70860-451832195-2: {"numFeatures":692}
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 5.4 KB, free 364.8 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.4 KB, free 364.8 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 138.96.200.169:44343 (size: 3.4 KB, free: 365.9 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 36 from broadcast at LogisticRegression.scala:600
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 10.9 KB, free 364.8 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 206.0 B, free 364.8 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 138.96.200.169:44343 (size: 206.0 B, free: 365.9 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 37 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 138.96.200.169:44343 in memory (size: 8.3 KB, free: 365.9 MB)
17/12/18 18:27:13 INFO DAGScheduler: Got job 17 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 17 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[37] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 17,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 17,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 37,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 37,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[37] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 18.5 KB, free 364.8 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 8.4 KB, free 364.8 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[37] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO BlockManager: Removing RDD 10
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 17.0 (TID 24)
17/12/18 18:27:13 INFO ContextCleaner: Cleaned RDD 10
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO ContextCleaner: Cleaned accumulator 50
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 17.0 (TID 24). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 24) in 25 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 17 (treeAggregate at LogisticRegression.scala:1894) finished in 0.024 s
17/12/18 18:27:13 INFO DAGScheduler: Job 17 finished: treeAggregate at LogisticRegression.scala:1894, took 0.039135 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(37) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 138.96.200.169:44343 in memory (size: 206.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 10.9 KB, free 365.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 2.4 KB, free 365.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 138.96.200.169:44343 (size: 2.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO ContextCleaner: Cleaned accumulator 52
17/12/18 18:27:13 INFO SparkContext: Created broadcast 39 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 18 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 18 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO ContextCleaner: Cleaned accumulator 51
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[38] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 18,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 18,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 38,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 38,
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[38] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 INFO ContextCleaner: Cleaned accumulator 48
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 138.96.200.169:44343 in memory (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO ContextCleaner: Cleaned accumulator 49
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 138.96.200.169:44343 in memory (size: 28.3 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 18.5 KB, free 365.4 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.4 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[38] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 18.0 (TID 25)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 18.0 (TID 25). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 25) in 5 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 18 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 18 finished: treeAggregate at LogisticRegression.scala:1894, took 0.016088 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(39) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 138.96.200.169:44343 in memory (size: 2.4 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 10.9 KB, free 365.4 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.4 KB, free 365.4 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 138.96.200.169:44343 (size: 2.4 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 41 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 19 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 19 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 19,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 19,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 39,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 39,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[39] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 18.5 KB, free 365.4 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.4 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 19.0 (TID 26)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 19.0 (TID 26). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 26) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 19 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 19 finished: treeAggregate at LogisticRegression.scala:1894, took 0.014440 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(41) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 138.96.200.169:44343 in memory (size: 2.4 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 0.007313
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.660286 (rel: 0.0337) 1.43920
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 10.9 KB, free 365.4 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 2.2 KB, free 365.4 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 138.96.200.169:44343 (size: 2.2 KB, free: 366.1 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 43 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 20 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 20 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[40] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 20,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 20,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 40,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 40,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[40] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 18.5 KB, free 365.4 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.4 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[40] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 20.0 (TID 27)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 20.0 (TID 27). 14114 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 27) in 8 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 20 (treeAggregate at LogisticRegression.scala:1894) finished in 0.007 s
17/12/18 18:27:13 INFO DAGScheduler: Job 20 finished: treeAggregate at LogisticRegression.scala:1894, took 0.015578 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(43) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 138.96.200.169:44343 in memory (size: 2.2 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.620935 (rel: 0.0596) 0.963391
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 10.9 KB, free 365.4 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 1895.0 B, free 365.4 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 138.96.200.169:44343 (size: 1895.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 45 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 21 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 21 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[41] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 21,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 21,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 41,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 41,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[41] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 18.5 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[41] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 21.0 (TID 28)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 21.0 (TID 28). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 28) in 5 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 21 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 21 finished: treeAggregate at LogisticRegression.scala:1894, took 0.012853 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(45) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 138.96.200.169:44343 in memory (size: 1895.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.612016 (rel: 0.0144) 0.485447
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 10.9 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 1524.0 B, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 138.96.200.169:44343 (size: 1524.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 47 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 22 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 22 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[42] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 22,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 22,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 42,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 42,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[42] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 18.5 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[42] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 22.0 (TID 29)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 22.0 (TID 29). 14114 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 29) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 22 (treeAggregate at LogisticRegression.scala:1894) finished in 0.005 s
17/12/18 18:27:13 INFO DAGScheduler: Job 22 finished: treeAggregate at LogisticRegression.scala:1894, took 0.010526 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(47) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 138.96.200.169:44343 in memory (size: 1524.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.605265 (rel: 0.0110) 0.389485
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 10.9 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 1316.0 B, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 138.96.200.169:44343 (size: 1316.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 49 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 23 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 23 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[43] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 23,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 23,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 43,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 43,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[43] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 18.5 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[43] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 23.0 (TID 30)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 23.0 (TID 30). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 30) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 23 (treeAggregate at LogisticRegression.scala:1894) finished in 0.007 s
17/12/18 18:27:13 INFO DAGScheduler: Job 23 finished: treeAggregate at LogisticRegression.scala:1894, took 0.014028 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(49) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 138.96.200.169:44343 in memory (size: 1316.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.602450 (rel: 0.00465) 0.291256
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 10.9 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 1164.0 B, free 365.3 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 138.96.200.169:44343 (size: 1164.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 51 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 24 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 24 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[44] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 24,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 24,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 44,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 44,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[44] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 18.5 KB, free 365.3 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[44] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 24.0 (TID 31)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 24.0 (TID 31). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 31) in 5 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 24 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 24 finished: treeAggregate at LogisticRegression.scala:1894, took 0.012674 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(51) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 138.96.200.169:44343 in memory (size: 1164.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 10.9 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 1209.0 B, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 138.96.200.169:44343 (size: 1209.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 53 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 25 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 25 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[45] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 25,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 25,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 45,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 45,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[45] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 18.5 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[45] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 25.0 (TID 32)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 25.0 (TID 32). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 32) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 25 (treeAggregate at LogisticRegression.scala:1894) finished in 0.004 s
17/12/18 18:27:13 INFO DAGScheduler: Job 25 finished: treeAggregate at LogisticRegression.scala:1894, took 0.008817 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(53) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 138.96.200.169:44343 in memory (size: 1209.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 0.5000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.596760 (rel: 0.00944) 0.399098
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 10.9 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 1151.0 B, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 138.96.200.169:44343 (size: 1151.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 55 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 26 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 26 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[46] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 26,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 26,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 46,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 46,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[46] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 18.5 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[46] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 26.0 (TID 33)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 26.0 (TID 33). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 33) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 26 (treeAggregate at LogisticRegression.scala:1894) finished in 0.007 s
17/12/18 18:27:13 INFO DAGScheduler: Job 26 finished: treeAggregate at LogisticRegression.scala:1894, took 0.011871 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(55) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 138.96.200.169:44343 in memory (size: 1151.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.594332 (rel: 0.00407) 0.256038
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 10.9 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 866.0 B, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 138.96.200.169:44343 (size: 866.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 57 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 27 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 27 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[47] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 27,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 27,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 47,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 47,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[47] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 18.5 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[47] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 27.0 (TID 34)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 27.0 (TID 34). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 34) in 3 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 27 (treeAggregate at LogisticRegression.scala:1894) finished in 0.001 s
17/12/18 18:27:13 INFO DAGScheduler: Job 27 finished: treeAggregate at LogisticRegression.scala:1894, took 0.009721 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(57) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 138.96.200.169:44343 in memory (size: 866.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.589024 (rel: 0.00893) 0.133408
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 10.9 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 578.0 B, free 365.2 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 138.96.200.169:44343 (size: 578.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 59 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 28 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 28 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[48] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 28,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 28,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 48,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 48,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[48] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 18.5 KB, free 365.2 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[48] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 28.0 (TID 35)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 28.0 (TID 35). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 35) in 5 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 28 (treeAggregate at LogisticRegression.scala:1894) finished in 0.005 s
17/12/18 18:27:13 INFO DAGScheduler: Job 28 finished: treeAggregate at LogisticRegression.scala:1894, took 0.015490 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(59) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 138.96.200.169:44343 in memory (size: 578.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 10.9 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 591.0 B, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 138.96.200.169:44343 (size: 591.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 61 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 29 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 29 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[49] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 29,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 29,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 49,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 49,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[49] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 18.5 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[49] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 29.0 (TID 36)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 29.0 (TID 36). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 36) in 6 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 29 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 29 finished: treeAggregate at LogisticRegression.scala:1894, took 0.012918 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(61) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 138.96.200.169:44343 in memory (size: 591.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 10.9 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 642.0 B, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 138.96.200.169:44343 (size: 642.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 63 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 30 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 30 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[50] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 30,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 30,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 50,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 50,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[50] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 18.5 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[50] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 30.0 (TID 37)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 30.0 (TID 37). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 37) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 30 (treeAggregate at LogisticRegression.scala:1894) finished in 0.006 s
17/12/18 18:27:13 INFO DAGScheduler: Job 30 finished: treeAggregate at LogisticRegression.scala:1894, took 0.010932 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(63) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 138.96.200.169:44343 in memory (size: 642.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 10.9 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 737.0 B, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 138.96.200.169:44343 (size: 737.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 65 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 31 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 31 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[51] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 51,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 51,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[51] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 18.5 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[51] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 31.0 (TID 38)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 31.0 (TID 38). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 38) in 5 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 31 (treeAggregate at LogisticRegression.scala:1894) finished in 0.005 s
17/12/18 18:27:13 INFO DAGScheduler: Job 31 finished: treeAggregate at LogisticRegression.scala:1894, took 0.010714 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(65) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 138.96.200.169:44343 in memory (size: 737.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO OWLQN: Step Size: 0.1250
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.587920 (rel: 0.00187) 0.213857
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 10.9 KB, free 365.1 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 688.0 B, free 365.1 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 138.96.200.169:44343 (size: 688.0 B, free: 366.0 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 67 from broadcast at LogisticRegression.scala:1881
17/12/18 18:27:13 INFO LogisticAggregator: Multinomial logistic regression for binary classification yields separate coefficients for positive and negative classes. When no regularization is applied, theresult will be effectively the same as binary logistic regression. When regularizationis applied, multinomial loss will produce a result different from binary loss.
17/12/18 18:27:13 INFO SparkContext: Starting job: treeAggregate at LogisticRegression.scala:1894
17/12/18 18:27:13 INFO DAGScheduler: Got job 32 (treeAggregate at LogisticRegression.scala:1894) with 1 output partitions
17/12/18 18:27:13 INFO DAGScheduler: Final stage: ResultStage 32 (treeAggregate at LogisticRegression.scala:1894)
17/12/18 18:27:13 INFO DAGScheduler: Parents of final stage: List()
17/12/18 18:27:13 INFO DAGScheduler: Missing parents: List()
17/12/18 18:27:13 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[52] at treeAggregate at LogisticRegression.scala:1894), which has no missing parents
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || {
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   name: ResultStage 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   final_rdd_id: 52,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   stage type: ResultStage,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   rdds: [
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 52,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[52] at treeAggregate at LogisticRegression.scala:1894,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 35}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 35,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[35] at map at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(disk, memory, deserialized, 1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 34}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 34,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[34] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 33}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 33,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[33] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 32}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 32,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: MapPartitionsRDD[32] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||          {type: narrow, rddid: 31}
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     },
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     {id: 31,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      name: FileScanRDD[31] at rdd at LogisticRegression.scala:495,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      storageLevel: StorageLevel(1 replicas),
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      partitions_number: 1,
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      deps: [ 
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||      ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||     }
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO ||   ]
17/12/18 18:27:13 WARN DAGScheduler: || DAGINFO || }
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 18.5 KB, free 365.0 MB)
17/12/18 18:27:13 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 8.4 KB, free 365.0 MB)
17/12/18 18:27:13 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 138.96.200.169:44343 (size: 8.4 KB, free: 365.9 MB)
17/12/18 18:27:13 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1067
17/12/18 18:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[52] at treeAggregate at LogisticRegression.scala:1894) (first 15 tasks are for partitions Vector(0))
17/12/18 18:27:13 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
17/12/18 18:27:13 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 5316 bytes)
17/12/18 18:27:13 INFO Executor: Running task 0.0 in stage 32.0 (TID 39)
17/12/18 18:27:13 INFO BlockManager: Found block rdd_35_0 locally
17/12/18 18:27:13 INFO Executor: Finished task 0.0 in stage 32.0 (TID 39). 14071 bytes result sent to driver
17/12/18 18:27:13 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 39) in 4 ms on localhost (executor driver) (1/1)
17/12/18 18:27:13 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/12/18 18:27:13 INFO DAGScheduler: ResultStage 32 (treeAggregate at LogisticRegression.scala:1894) finished in 0.004 s
17/12/18 18:27:13 INFO DAGScheduler: Job 32 finished: treeAggregate at LogisticRegression.scala:1894, took 0.009904 s
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(67) (from destroy at LogisticRegression.scala:1935)
17/12/18 18:27:13 INFO OWLQN: Step Size: 1.000
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 138.96.200.169:44343 in memory (size: 688.0 B, free: 365.9 MB)
17/12/18 18:27:13 INFO OWLQN: Val and Grad Norm: 0.586141 (rel: 0.00302) 0.168430
17/12/18 18:27:13 INFO OWLQN: Converged because max iterations reached
17/12/18 18:27:13 INFO TorrentBroadcast: Destroying Broadcast(36) (from destroy at LogisticRegression.scala:796)
17/12/18 18:27:13 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 138.96.200.169:44343 in memory (size: 3.4 KB, free: 365.9 MB)
17/12/18 18:27:13 INFO MapPartitionsRDD: Removing RDD 35 from persistence list
17/12/18 18:27:13 INFO BlockManager: Removing RDD 35
17/12/18 18:27:13 INFO Instrumentation: LogisticRegression-logreg_268094a70860-451832195-2: training finished
17/12/18 18:27:14 INFO AbstractConnector: Stopped Spark@710d188a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17/12/18 18:27:14 INFO SparkUI: Stopped Spark web UI at http://138.96.200.169:4040
17/12/18 18:27:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/18 18:27:14 INFO MemoryStore: MemoryStore cleared
17/12/18 18:27:14 INFO BlockManager: BlockManager stopped
17/12/18 18:27:14 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/18 18:27:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/18 18:27:14 INFO SparkContext: Successfully stopped SparkContext
17/12/18 18:27:14 INFO ShutdownHookManager: Shutdown hook called
17/12/18 18:27:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-3cbdd6f7-feaf-427b-8191-14517a72340c
